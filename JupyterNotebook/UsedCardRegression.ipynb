{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing All The Necessary Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame,Series\n",
    "from sklearn.decomposition import PCA\n",
    "import  matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Year</th>\n",
       "      <th>Kilometers_Driven</th>\n",
       "      <th>Fuel_Type</th>\n",
       "      <th>Transmission</th>\n",
       "      <th>Owner_Type</th>\n",
       "      <th>Mileage</th>\n",
       "      <th>Engine</th>\n",
       "      <th>Power</th>\n",
       "      <th>Seats</th>\n",
       "      <th>New_Price</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Honda Jazz V</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>2011</td>\n",
       "      <td>46000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>18.20</td>\n",
       "      <td>1199</td>\n",
       "      <td>88.70</td>\n",
       "      <td>5</td>\n",
       "      <td>8.61</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Toyota Innova Crysta 2.8 GX AT 8S</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2016</td>\n",
       "      <td>36000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>11.36</td>\n",
       "      <td>2755</td>\n",
       "      <td>171.50</td>\n",
       "      <td>8</td>\n",
       "      <td>21.00</td>\n",
       "      <td>17.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Maruti Ciaz Zeta</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2018</td>\n",
       "      <td>25692</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>21.56</td>\n",
       "      <td>1462</td>\n",
       "      <td>103.25</td>\n",
       "      <td>5</td>\n",
       "      <td>10.65</td>\n",
       "      <td>9.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mitsubishi Pajero Sport 4X4</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>2014</td>\n",
       "      <td>110000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>13.50</td>\n",
       "      <td>2477</td>\n",
       "      <td>175.56</td>\n",
       "      <td>7</td>\n",
       "      <td>32.01</td>\n",
       "      <td>15.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMW 3 Series 320d</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2014</td>\n",
       "      <td>32982</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995</td>\n",
       "      <td>190.00</td>\n",
       "      <td>5</td>\n",
       "      <td>47.87</td>\n",
       "      <td>18.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Honda WRV i-VTEC VX</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2018</td>\n",
       "      <td>37430</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>17.50</td>\n",
       "      <td>1199</td>\n",
       "      <td>88.70</td>\n",
       "      <td>5</td>\n",
       "      <td>10.57</td>\n",
       "      <td>9.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Volkswagen Vento 1.6 Highline</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>2014</td>\n",
       "      <td>55431</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second</td>\n",
       "      <td>16.09</td>\n",
       "      <td>1598</td>\n",
       "      <td>103.50</td>\n",
       "      <td>5</td>\n",
       "      <td>12.33</td>\n",
       "      <td>6.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maruti Vitara Brezza ZDi Plus</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2018</td>\n",
       "      <td>50075</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>24.30</td>\n",
       "      <td>1248</td>\n",
       "      <td>88.50</td>\n",
       "      <td>5</td>\n",
       "      <td>11.12</td>\n",
       "      <td>8.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hyundai Elantra 2.0 SX Option AT</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>2014</td>\n",
       "      <td>46374</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>14.62</td>\n",
       "      <td>1999</td>\n",
       "      <td>149.92</td>\n",
       "      <td>5</td>\n",
       "      <td>23.64</td>\n",
       "      <td>8.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Toyota Innova Crysta 2.7 GX MT</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>2012</td>\n",
       "      <td>70704</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>11.25</td>\n",
       "      <td>2694</td>\n",
       "      <td>163.70</td>\n",
       "      <td>7</td>\n",
       "      <td>18.64</td>\n",
       "      <td>10.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Toyota Innova Crysta 2.4 GX MT 8S</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2017</td>\n",
       "      <td>22000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>13.68</td>\n",
       "      <td>2393</td>\n",
       "      <td>147.80</td>\n",
       "      <td>8</td>\n",
       "      <td>19.34</td>\n",
       "      <td>16.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mercedes-Benz C-Class Progressive C 220d</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>2019</td>\n",
       "      <td>15369</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1950</td>\n",
       "      <td>194.00</td>\n",
       "      <td>5</td>\n",
       "      <td>49.14</td>\n",
       "      <td>35.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Hyundai Creta 1.4 E Plus Diesel</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2018</td>\n",
       "      <td>13000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>22.10</td>\n",
       "      <td>1396</td>\n",
       "      <td>88.70</td>\n",
       "      <td>5</td>\n",
       "      <td>11.81</td>\n",
       "      <td>10.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Nissan Terrano XV D Pre</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2015</td>\n",
       "      <td>56066</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>19.64</td>\n",
       "      <td>1461</td>\n",
       "      <td>108.50</td>\n",
       "      <td>5</td>\n",
       "      <td>16.65</td>\n",
       "      <td>6.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Tata Zest Revotron 1.2T XE</td>\n",
       "      <td>Pune</td>\n",
       "      <td>2017</td>\n",
       "      <td>70000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>17.57</td>\n",
       "      <td>1193</td>\n",
       "      <td>88.70</td>\n",
       "      <td>5</td>\n",
       "      <td>6.67</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Datsun GO Plus T Petrol</td>\n",
       "      <td>Pune</td>\n",
       "      <td>2017</td>\n",
       "      <td>32851</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>19.83</td>\n",
       "      <td>1198</td>\n",
       "      <td>67.00</td>\n",
       "      <td>7</td>\n",
       "      <td>6.45</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Toyota Etios Liva 1.2 G</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2014</td>\n",
       "      <td>53000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>17.71</td>\n",
       "      <td>1197</td>\n",
       "      <td>78.90</td>\n",
       "      <td>5</td>\n",
       "      <td>6.41</td>\n",
       "      <td>3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Toyota Innova Crysta 2.8 ZX AT</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2017</td>\n",
       "      <td>30000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>11.36</td>\n",
       "      <td>2755</td>\n",
       "      <td>171.50</td>\n",
       "      <td>7</td>\n",
       "      <td>27.00</td>\n",
       "      <td>19.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maruti Eeco 5 Seater AC</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>2017</td>\n",
       "      <td>38053</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>15.10</td>\n",
       "      <td>1196</td>\n",
       "      <td>73.00</td>\n",
       "      <td>5</td>\n",
       "      <td>5.13</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BMW 3 Series 320d Luxury Line</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2015</td>\n",
       "      <td>56087</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>22.69</td>\n",
       "      <td>1995</td>\n",
       "      <td>190.00</td>\n",
       "      <td>5</td>\n",
       "      <td>54.43</td>\n",
       "      <td>20.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Nissan Micra XV CVT</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>2016</td>\n",
       "      <td>39882</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>19.15</td>\n",
       "      <td>1198</td>\n",
       "      <td>75.94</td>\n",
       "      <td>5</td>\n",
       "      <td>9.51</td>\n",
       "      <td>6.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Maruti Vitara Brezza VDi</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2016</td>\n",
       "      <td>55000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>24.30</td>\n",
       "      <td>1248</td>\n",
       "      <td>88.50</td>\n",
       "      <td>5</td>\n",
       "      <td>9.96</td>\n",
       "      <td>7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hyundai Verna CRDi 1.6 SX Option</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>2018</td>\n",
       "      <td>10250</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>22.00</td>\n",
       "      <td>1582</td>\n",
       "      <td>126.20</td>\n",
       "      <td>5</td>\n",
       "      <td>14.44</td>\n",
       "      <td>10.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Maruti Alto K10 VXI</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2014</td>\n",
       "      <td>88500</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>24.07</td>\n",
       "      <td>998</td>\n",
       "      <td>67.10</td>\n",
       "      <td>5</td>\n",
       "      <td>4.53</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Mercedes-Benz SLC 43 AMG</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>2017</td>\n",
       "      <td>13372</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>19.00</td>\n",
       "      <td>2996</td>\n",
       "      <td>362.07</td>\n",
       "      <td>2</td>\n",
       "      <td>95.04</td>\n",
       "      <td>54.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hyundai Verna VTVT 1.6 AT SX Plus</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2019</td>\n",
       "      <td>12645</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1591</td>\n",
       "      <td>121.30</td>\n",
       "      <td>5</td>\n",
       "      <td>13.49</td>\n",
       "      <td>11.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Audi RS5 Coupe</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2013</td>\n",
       "      <td>23000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>11.05</td>\n",
       "      <td>2894</td>\n",
       "      <td>444.00</td>\n",
       "      <td>4</td>\n",
       "      <td>1.28</td>\n",
       "      <td>37.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>BMW 3 Series Luxury Line</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2014</td>\n",
       "      <td>48000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>21.76</td>\n",
       "      <td>1995</td>\n",
       "      <td>190.00</td>\n",
       "      <td>5</td>\n",
       "      <td>60.88</td>\n",
       "      <td>26.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hyundai Grand i10 1.2 Kappa Asta</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>2019</td>\n",
       "      <td>1000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>81.86</td>\n",
       "      <td>5</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>BMW 5 Series 520d Luxury Line</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2014</td>\n",
       "      <td>51000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>22.48</td>\n",
       "      <td>1995</td>\n",
       "      <td>190.00</td>\n",
       "      <td>5</td>\n",
       "      <td>69.47</td>\n",
       "      <td>33.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>Renault KWID RXT Optional</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2018</td>\n",
       "      <td>1617</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>25.17</td>\n",
       "      <td>799</td>\n",
       "      <td>53.30</td>\n",
       "      <td>5</td>\n",
       "      <td>4.78</td>\n",
       "      <td>4.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>Honda City i-DTEC SV</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2015</td>\n",
       "      <td>82000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>25.60</td>\n",
       "      <td>1498</td>\n",
       "      <td>98.60</td>\n",
       "      <td>5</td>\n",
       "      <td>13.33</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>Hyundai Grand i10 1.2 Kappa Sportz</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2016</td>\n",
       "      <td>28147</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>18.90</td>\n",
       "      <td>1197</td>\n",
       "      <td>81.86</td>\n",
       "      <td>5</td>\n",
       "      <td>6.62</td>\n",
       "      <td>5.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>Renault Duster 110PS Diesel RxZ</td>\n",
       "      <td>Bangalore</td>\n",
       "      <td>2014</td>\n",
       "      <td>70000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>19.60</td>\n",
       "      <td>1461</td>\n",
       "      <td>108.45</td>\n",
       "      <td>5</td>\n",
       "      <td>15.01</td>\n",
       "      <td>8.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>Skoda Superb L&amp;K 2.0 TDI AT</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2017</td>\n",
       "      <td>35000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>18.19</td>\n",
       "      <td>1968</td>\n",
       "      <td>174.50</td>\n",
       "      <td>5</td>\n",
       "      <td>40.73</td>\n",
       "      <td>25.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>Maruti Vitara Brezza ZDi Plus Dual Tone</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2017</td>\n",
       "      <td>21248</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>24.30</td>\n",
       "      <td>1248</td>\n",
       "      <td>88.50</td>\n",
       "      <td>5</td>\n",
       "      <td>11.75</td>\n",
       "      <td>9.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>Mercedes-Benz C-Class Progressive C 220d</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2019</td>\n",
       "      <td>4000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1950</td>\n",
       "      <td>194.00</td>\n",
       "      <td>5</td>\n",
       "      <td>49.14</td>\n",
       "      <td>35.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>Mitsubishi Pajero Sport 4X4</td>\n",
       "      <td>Jaipur</td>\n",
       "      <td>2014</td>\n",
       "      <td>70000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second</td>\n",
       "      <td>13.50</td>\n",
       "      <td>2477</td>\n",
       "      <td>175.56</td>\n",
       "      <td>7</td>\n",
       "      <td>33.21</td>\n",
       "      <td>11.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>Hyundai Creta 1.6 SX</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2018</td>\n",
       "      <td>9900</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>15.80</td>\n",
       "      <td>1591</td>\n",
       "      <td>121.30</td>\n",
       "      <td>5</td>\n",
       "      <td>14.72</td>\n",
       "      <td>13.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>803</th>\n",
       "      <td>Tata Zest Quadrajet 1.3 75PS XE</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2016</td>\n",
       "      <td>125000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>22.95</td>\n",
       "      <td>1248</td>\n",
       "      <td>74.00</td>\n",
       "      <td>5</td>\n",
       "      <td>8.09</td>\n",
       "      <td>4.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>Renault Duster 110PS Diesel RxZ AMT</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>2017</td>\n",
       "      <td>34686</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>19.60</td>\n",
       "      <td>1461</td>\n",
       "      <td>108.45</td>\n",
       "      <td>5</td>\n",
       "      <td>15.05</td>\n",
       "      <td>11.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>Skoda Superb Style 2.0 TDI AT</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2017</td>\n",
       "      <td>42000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>18.19</td>\n",
       "      <td>1968</td>\n",
       "      <td>174.50</td>\n",
       "      <td>5</td>\n",
       "      <td>36.94</td>\n",
       "      <td>22.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>Mahindra NuvoSport N6</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>2018</td>\n",
       "      <td>37323</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second</td>\n",
       "      <td>17.45</td>\n",
       "      <td>1493</td>\n",
       "      <td>100.00</td>\n",
       "      <td>7</td>\n",
       "      <td>10.52</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>Maruti Dzire LDI</td>\n",
       "      <td>Pune</td>\n",
       "      <td>2018</td>\n",
       "      <td>27000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>28.40</td>\n",
       "      <td>1248</td>\n",
       "      <td>73.75</td>\n",
       "      <td>5</td>\n",
       "      <td>7.91</td>\n",
       "      <td>5.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>Renault KWID RXL</td>\n",
       "      <td>Pune</td>\n",
       "      <td>2018</td>\n",
       "      <td>1000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>25.17</td>\n",
       "      <td>799</td>\n",
       "      <td>53.30</td>\n",
       "      <td>5</td>\n",
       "      <td>4.45</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>Maruti Ertiga VXI AT Petrol</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2017</td>\n",
       "      <td>28000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>18.69</td>\n",
       "      <td>1462</td>\n",
       "      <td>103.00</td>\n",
       "      <td>7</td>\n",
       "      <td>10.65</td>\n",
       "      <td>8.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>Toyota Corolla Altis 1.8 G CVT</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>2010</td>\n",
       "      <td>109703</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>Second</td>\n",
       "      <td>14.28</td>\n",
       "      <td>1798</td>\n",
       "      <td>138.03</td>\n",
       "      <td>5</td>\n",
       "      <td>20.46</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>Hyundai Verna VTVT 1.6 AT SX Option</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2018</td>\n",
       "      <td>8000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1591</td>\n",
       "      <td>121.30</td>\n",
       "      <td>5</td>\n",
       "      <td>15.14</td>\n",
       "      <td>12.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>Mercedes-Benz GLA Class 200 Sport</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2018</td>\n",
       "      <td>17773</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>13.70</td>\n",
       "      <td>1991</td>\n",
       "      <td>183.00</td>\n",
       "      <td>5</td>\n",
       "      <td>39.22</td>\n",
       "      <td>26.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813</th>\n",
       "      <td>Volkswagen Polo 1.5 TDI Highline</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>2013</td>\n",
       "      <td>65000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>Second</td>\n",
       "      <td>20.14</td>\n",
       "      <td>1498</td>\n",
       "      <td>88.00</td>\n",
       "      <td>5</td>\n",
       "      <td>10.15</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814</th>\n",
       "      <td>Volkswagen Ameo 1.2 MPI Highline</td>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2016</td>\n",
       "      <td>34005</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>17.00</td>\n",
       "      <td>1198</td>\n",
       "      <td>73.75</td>\n",
       "      <td>5</td>\n",
       "      <td>8.09</td>\n",
       "      <td>5.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815</th>\n",
       "      <td>Maruti Baleno Alpha</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2017</td>\n",
       "      <td>6000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>21.40</td>\n",
       "      <td>1197</td>\n",
       "      <td>83.10</td>\n",
       "      <td>5</td>\n",
       "      <td>8.78</td>\n",
       "      <td>7.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>816</th>\n",
       "      <td>Tata Hexa XTA</td>\n",
       "      <td>Jaipur</td>\n",
       "      <td>2016</td>\n",
       "      <td>39000</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>First</td>\n",
       "      <td>17.60</td>\n",
       "      <td>2179</td>\n",
       "      <td>153.86</td>\n",
       "      <td>7</td>\n",
       "      <td>21.00</td>\n",
       "      <td>13.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>Tata Tiago 1.2 Revotron XT</td>\n",
       "      <td>Kochi</td>\n",
       "      <td>2017</td>\n",
       "      <td>15386</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>23.84</td>\n",
       "      <td>1199</td>\n",
       "      <td>84.00</td>\n",
       "      <td>5</td>\n",
       "      <td>5.56</td>\n",
       "      <td>5.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>Audi A4 35 TDI Premium Plus</td>\n",
       "      <td>Coimbatore</td>\n",
       "      <td>2013</td>\n",
       "      <td>58629</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Automatic</td>\n",
       "      <td>Second</td>\n",
       "      <td>18.25</td>\n",
       "      <td>1968</td>\n",
       "      <td>187.74</td>\n",
       "      <td>5</td>\n",
       "      <td>53.14</td>\n",
       "      <td>16.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>819</th>\n",
       "      <td>Tata Bolt Revotron XT</td>\n",
       "      <td>Chennai</td>\n",
       "      <td>2016</td>\n",
       "      <td>10000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>17.57</td>\n",
       "      <td>1193</td>\n",
       "      <td>88.70</td>\n",
       "      <td>5</td>\n",
       "      <td>7.77</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>Volkswagen Vento 1.6 Highline</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>2011</td>\n",
       "      <td>38000</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>16.09</td>\n",
       "      <td>1598</td>\n",
       "      <td>103.50</td>\n",
       "      <td>5</td>\n",
       "      <td>11.91</td>\n",
       "      <td>3.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821</th>\n",
       "      <td>Maruti Vitara Brezza VDi</td>\n",
       "      <td>Pune</td>\n",
       "      <td>2016</td>\n",
       "      <td>37208</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>24.30</td>\n",
       "      <td>1248</td>\n",
       "      <td>88.50</td>\n",
       "      <td>5</td>\n",
       "      <td>9.93</td>\n",
       "      <td>7.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>822</th>\n",
       "      <td>Honda Brio 1.2 VX MT</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>2013</td>\n",
       "      <td>33746</td>\n",
       "      <td>Petrol</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>18.50</td>\n",
       "      <td>1198</td>\n",
       "      <td>86.80</td>\n",
       "      <td>5</td>\n",
       "      <td>6.63</td>\n",
       "      <td>3.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>Maruti Swift VDI</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>2014</td>\n",
       "      <td>27365</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>Manual</td>\n",
       "      <td>First</td>\n",
       "      <td>28.40</td>\n",
       "      <td>1248</td>\n",
       "      <td>74.00</td>\n",
       "      <td>5</td>\n",
       "      <td>7.88</td>\n",
       "      <td>4.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>824 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Name    Location  Year  \\\n",
       "0                                Honda Jazz V     Chennai  2011   \n",
       "1           Toyota Innova Crysta 2.8 GX AT 8S      Mumbai  2016   \n",
       "2                            Maruti Ciaz Zeta       Kochi  2018   \n",
       "3                 Mitsubishi Pajero Sport 4X4       Delhi  2014   \n",
       "4                           BMW 3 Series 320d       Kochi  2014   \n",
       "5                         Honda WRV i-VTEC VX       Kochi  2018   \n",
       "6               Volkswagen Vento 1.6 Highline  Coimbatore  2014   \n",
       "7               Maruti Vitara Brezza ZDi Plus       Kochi  2018   \n",
       "8            Hyundai Elantra 2.0 SX Option AT   Bangalore  2014   \n",
       "9              Toyota Innova Crysta 2.7 GX MT   Bangalore  2012   \n",
       "10          Toyota Innova Crysta 2.4 GX MT 8S      Mumbai  2017   \n",
       "11   Mercedes-Benz C-Class Progressive C 220d  Coimbatore  2019   \n",
       "12            Hyundai Creta 1.4 E Plus Diesel      Mumbai  2018   \n",
       "13                    Nissan Terrano XV D Pre       Kochi  2015   \n",
       "14                 Tata Zest Revotron 1.2T XE        Pune  2017   \n",
       "15                    Datsun GO Plus T Petrol        Pune  2017   \n",
       "16                    Toyota Etios Liva 1.2 G   Ahmedabad  2014   \n",
       "17             Toyota Innova Crysta 2.8 ZX AT      Mumbai  2017   \n",
       "18                    Maruti Eeco 5 Seater AC  Coimbatore  2017   \n",
       "19              BMW 3 Series 320d Luxury Line      Mumbai  2015   \n",
       "20                        Nissan Micra XV CVT  Coimbatore  2016   \n",
       "21                   Maruti Vitara Brezza VDi      Mumbai  2016   \n",
       "22           Hyundai Verna CRDi 1.6 SX Option     Kolkata  2018   \n",
       "23                        Maruti Alto K10 VXI   Hyderabad  2014   \n",
       "24                   Mercedes-Benz SLC 43 AMG     Kolkata  2017   \n",
       "25          Hyundai Verna VTVT 1.6 AT SX Plus       Kochi  2019   \n",
       "26                             Audi RS5 Coupe      Mumbai  2013   \n",
       "27                   BMW 3 Series Luxury Line      Mumbai  2014   \n",
       "28           Hyundai Grand i10 1.2 Kappa Asta     Kolkata  2019   \n",
       "29              BMW 5 Series 520d Luxury Line   Hyderabad  2014   \n",
       "..                                        ...         ...   ...   \n",
       "794                 Renault KWID RXT Optional      Mumbai  2018   \n",
       "795                      Honda City i-DTEC SV   Hyderabad  2015   \n",
       "796        Hyundai Grand i10 1.2 Kappa Sportz       Kochi  2016   \n",
       "797           Renault Duster 110PS Diesel RxZ   Bangalore  2014   \n",
       "798               Skoda Superb L&K 2.0 TDI AT      Mumbai  2017   \n",
       "799   Maruti Vitara Brezza ZDi Plus Dual Tone       Kochi  2017   \n",
       "800  Mercedes-Benz C-Class Progressive C 220d   Ahmedabad  2019   \n",
       "801               Mitsubishi Pajero Sport 4X4      Jaipur  2014   \n",
       "802                      Hyundai Creta 1.6 SX   Hyderabad  2018   \n",
       "803           Tata Zest Quadrajet 1.3 75PS XE   Hyderabad  2016   \n",
       "804       Renault Duster 110PS Diesel RxZ AMT  Coimbatore  2017   \n",
       "805             Skoda Superb Style 2.0 TDI AT      Mumbai  2017   \n",
       "806                     Mahindra NuvoSport N6  Coimbatore  2018   \n",
       "807                          Maruti Dzire LDI        Pune  2018   \n",
       "808                          Renault KWID RXL        Pune  2018   \n",
       "809               Maruti Ertiga VXI AT Petrol      Mumbai  2017   \n",
       "810            Toyota Corolla Altis 1.8 G CVT       Delhi  2010   \n",
       "811       Hyundai Verna VTVT 1.6 AT SX Option      Mumbai  2018   \n",
       "812         Mercedes-Benz GLA Class 200 Sport       Kochi  2018   \n",
       "813          Volkswagen Polo 1.5 TDI Highline   Hyderabad  2013   \n",
       "814          Volkswagen Ameo 1.2 MPI Highline   Ahmedabad  2016   \n",
       "815                       Maruti Baleno Alpha      Mumbai  2017   \n",
       "816                             Tata Hexa XTA      Jaipur  2016   \n",
       "817                Tata Tiago 1.2 Revotron XT       Kochi  2017   \n",
       "818               Audi A4 35 TDI Premium Plus  Coimbatore  2013   \n",
       "819                     Tata Bolt Revotron XT     Chennai  2016   \n",
       "820             Volkswagen Vento 1.6 Highline      Mumbai  2011   \n",
       "821                  Maruti Vitara Brezza VDi        Pune  2016   \n",
       "822                      Honda Brio 1.2 VX MT       Delhi  2013   \n",
       "823                          Maruti Swift VDI       Delhi  2014   \n",
       "\n",
       "     Kilometers_Driven Fuel_Type Transmission Owner_Type  Mileage  Engine  \\\n",
       "0                46000    Petrol       Manual      First    18.20    1199   \n",
       "1                36000    Diesel    Automatic      First    11.36    2755   \n",
       "2                25692    Petrol       Manual      First    21.56    1462   \n",
       "3               110000    Diesel       Manual      First    13.50    2477   \n",
       "4                32982    Diesel    Automatic      First    22.69    1995   \n",
       "5                37430    Petrol       Manual      First    17.50    1199   \n",
       "6                55431    Petrol       Manual     Second    16.09    1598   \n",
       "7                50075    Diesel       Manual      First    24.30    1248   \n",
       "8                46374    Petrol    Automatic      First    14.62    1999   \n",
       "9                70704    Petrol       Manual      First    11.25    2694   \n",
       "10               22000    Diesel       Manual      First    13.68    2393   \n",
       "11               15369    Diesel    Automatic      First     0.00    1950   \n",
       "12               13000    Diesel       Manual      First    22.10    1396   \n",
       "13               56066    Diesel       Manual      First    19.64    1461   \n",
       "14               70000    Petrol       Manual      First    17.57    1193   \n",
       "15               32851    Petrol       Manual      First    19.83    1198   \n",
       "16               53000    Petrol       Manual      First    17.71    1197   \n",
       "17               30000    Diesel    Automatic      First    11.36    2755   \n",
       "18               38053    Petrol       Manual      First    15.10    1196   \n",
       "19               56087    Diesel    Automatic      First    22.69    1995   \n",
       "20               39882    Petrol    Automatic      First    19.15    1198   \n",
       "21               55000    Diesel       Manual      First    24.30    1248   \n",
       "22               10250    Diesel       Manual      First    22.00    1582   \n",
       "23               88500    Petrol       Manual      First    24.07     998   \n",
       "24               13372    Petrol    Automatic      First    19.00    2996   \n",
       "25               12645    Petrol    Automatic      First    17.00    1591   \n",
       "26               23000    Petrol    Automatic      First    11.05    2894   \n",
       "27               48000    Diesel    Automatic      First    21.76    1995   \n",
       "28                1000    Petrol       Manual      First    18.90    1197   \n",
       "29               51000    Diesel    Automatic      First    22.48    1995   \n",
       "..                 ...       ...          ...        ...      ...     ...   \n",
       "794               1617    Petrol       Manual      First    25.17     799   \n",
       "795              82000    Diesel       Manual      First    25.60    1498   \n",
       "796              28147    Petrol       Manual      First    18.90    1197   \n",
       "797              70000    Diesel       Manual      First    19.60    1461   \n",
       "798              35000    Diesel    Automatic      First    18.19    1968   \n",
       "799              21248    Diesel       Manual      First    24.30    1248   \n",
       "800               4000    Diesel    Automatic      First     0.00    1950   \n",
       "801              70000    Diesel       Manual     Second    13.50    2477   \n",
       "802               9900    Petrol       Manual      First    15.80    1591   \n",
       "803             125000    Diesel       Manual      First    22.95    1248   \n",
       "804              34686    Diesel    Automatic      First    19.60    1461   \n",
       "805              42000    Diesel    Automatic      First    18.19    1968   \n",
       "806              37323    Diesel       Manual     Second    17.45    1493   \n",
       "807              27000    Diesel       Manual      First    28.40    1248   \n",
       "808               1000    Petrol       Manual      First    25.17     799   \n",
       "809              28000    Petrol    Automatic      First    18.69    1462   \n",
       "810             109703    Petrol    Automatic     Second    14.28    1798   \n",
       "811               8000    Petrol    Automatic      First    17.00    1591   \n",
       "812              17773    Petrol    Automatic      First    13.70    1991   \n",
       "813              65000    Diesel       Manual     Second    20.14    1498   \n",
       "814              34005    Petrol       Manual      First    17.00    1198   \n",
       "815               6000    Petrol       Manual      First    21.40    1197   \n",
       "816              39000    Diesel    Automatic      First    17.60    2179   \n",
       "817              15386    Petrol       Manual      First    23.84    1199   \n",
       "818              58629    Diesel    Automatic     Second    18.25    1968   \n",
       "819              10000    Petrol       Manual      First    17.57    1193   \n",
       "820              38000    Petrol       Manual      First    16.09    1598   \n",
       "821              37208    Diesel       Manual      First    24.30    1248   \n",
       "822              33746    Petrol       Manual      First    18.50    1198   \n",
       "823              27365    Diesel       Manual      First    28.40    1248   \n",
       "\n",
       "      Power  Seats  New_Price  Price  \n",
       "0     88.70      5       8.61   4.50  \n",
       "1    171.50      8      21.00  17.50  \n",
       "2    103.25      5      10.65   9.95  \n",
       "3    175.56      7      32.01  15.00  \n",
       "4    190.00      5      47.87  18.55  \n",
       "5     88.70      5      10.57   9.90  \n",
       "6    103.50      5      12.33   6.98  \n",
       "7     88.50      5      11.12   8.63  \n",
       "8    149.92      5      23.64   8.85  \n",
       "9    163.70      7      18.64  10.95  \n",
       "10   147.80      8      19.34  16.50  \n",
       "11   194.00      5      49.14  35.67  \n",
       "12    88.70      5      11.81  10.50  \n",
       "13   108.50      5      16.65   6.92  \n",
       "14    88.70      5       6.67   4.50  \n",
       "15    67.00      7       6.45   3.95  \n",
       "16    78.90      5       6.41   3.90  \n",
       "17   171.50      7      27.00  19.25  \n",
       "18    73.00      5       5.13   3.91  \n",
       "19   190.00      5      54.43  20.75  \n",
       "20    75.94      5       9.51   6.55  \n",
       "21    88.50      5       9.96   7.50  \n",
       "22   126.20      5      14.44  10.95  \n",
       "23    67.10      5       4.53   2.60  \n",
       "24   362.07      2      95.04  54.00  \n",
       "25   121.30      5      13.49  11.50  \n",
       "26   444.00      4       1.28  37.00  \n",
       "27   190.00      5      60.88  26.50  \n",
       "28    81.86      5       7.39   6.50  \n",
       "29   190.00      5      69.47  33.50  \n",
       "..      ...    ...        ...    ...  \n",
       "794   53.30      5       4.78   4.29  \n",
       "795   98.60      5      13.33   8.00  \n",
       "796   81.86      5       6.62   5.07  \n",
       "797  108.45      5      15.01   8.25  \n",
       "798  174.50      5      40.73  25.75  \n",
       "799   88.50      5      11.75   9.44  \n",
       "800  194.00      5      49.14  35.00  \n",
       "801  175.56      7      33.21  11.85  \n",
       "802  121.30      5      14.72  13.20  \n",
       "803   74.00      5       8.09   4.60  \n",
       "804  108.45      5      15.05  11.86  \n",
       "805  174.50      5      36.94  22.99  \n",
       "806  100.00      7      10.52   7.29  \n",
       "807   73.75      5       7.91   5.90  \n",
       "808   53.30      5       4.45   3.40  \n",
       "809  103.00      7      10.65   8.00  \n",
       "810  138.03      5      20.46   4.00  \n",
       "811  121.30      5      15.14  12.85  \n",
       "812  183.00      5      39.22  26.76  \n",
       "813   88.00      5      10.15   3.50  \n",
       "814   73.75      5       8.09   5.75  \n",
       "815   83.10      5       8.78   7.50  \n",
       "816  153.86      7      21.00  13.50  \n",
       "817   84.00      5       5.56   5.11  \n",
       "818  187.74      5      53.14  16.52  \n",
       "819   88.70      5       7.77   4.00  \n",
       "820  103.50      5      11.91   3.25  \n",
       "821   88.50      5       9.93   7.43  \n",
       "822   86.80      5       6.63   3.20  \n",
       "823   74.00      5       7.88   4.75  \n",
       "\n",
       "[824 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars=pd.read_excel('C:/Users/Amardeep/Desktop/PLASTICS DATASETS/Participants_Data_Used_Cars/Data_Train.xlsx')\n",
    "cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars_columns=cars.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot=pd.get_dummies(cars['Name'])\n",
    "# Drop column as it is now encoded\n",
    "cars = cars.drop('Name',axis = 1)\n",
    "# Join the encoded df\n",
    "cars = cars.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_hot=pd.get_dummies(cars['Location'])\n",
    "# Drop column as it is now encoded\n",
    "cars = cars.drop('Location',axis = 1)\n",
    "# Join the encoded df\n",
    "cars = cars.join(two_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_hot=pd.get_dummies(cars['Year'])\n",
    "# Drop column as it is now encoded\n",
    "cars = cars.drop('Year',axis = 1)\n",
    "# Join the encoded df\n",
    "cars = cars.join(three_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_hot=pd.get_dummies(cars['Fuel_Type'])\n",
    "# Drop column as it is now encoded\n",
    "cars = cars.drop('Fuel_Type',axis = 1)\n",
    "# Join the encoded df\n",
    "cars = cars.join(four_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_hot=pd.get_dummies(cars['Transmission'])\n",
    "# Drop column as it is now encoded\n",
    "cars = cars.drop('Transmission',axis = 1)\n",
    "# Join the encoded df\n",
    "cars = cars.join(five_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "six_hot=pd.get_dummies(cars['Owner_Type'])\n",
    "# Drop column as it is now encoded\n",
    "cars = cars.drop('Owner_Type',axis = 1)\n",
    "# Join the encoded df\n",
    "cars = cars.join(six_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars=cars.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cars.to_excel(\"C:/Users/Amardeep/Desktop/PLASTICS DATASETS/cars.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Import necessary modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cars.drop(columns='Price')\n",
    "y=cars['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 364)\n",
      "(247, 364)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow.python.keras.models",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-579d65d5d227>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow.python.keras.models"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named tensorflow",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-f7f283373e39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named tensorflow"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<ipython-input-38-36c33bc84be1>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-38-36c33bc84be1>\"\u001b[1;36m, line \u001b[1;32m22\u001b[0m\n\u001b[1;33m    def set_params(self, **params):\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    " def get_params(self, deep=True):\n",
    "        \"\"\"Get parameters for this estimator.\n",
    "        Parameters\n",
    "        ----------\n",
    "        deep : boolean, optional\n",
    "            If True, will return the parameters for this estimator and\n",
    "            contained subobjects that are estimators.\n",
    "        Returns\n",
    "        -------\n",
    "        params : mapping of string to any\n",
    "            Parameter names mapped to their values.\n",
    "        \"\"\"\n",
    "        out = dict()\n",
    "        for key in self._get_param_names():\n",
    "            value = getattr(self, key, None)\n",
    "            if deep and hasattr(value, 'get_params'):\n",
    "                deep_items = value.get_params().items()\n",
    "                out.update((key + '__' + k, val) for k, val in deep_items)\n",
    "            out[key] = value\n",
    "        return out\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set the parameters of this estimator.\n",
    "        The method works on simple estimators as well as on nested objects\n",
    "        (such as pipelines). The latter have parameters of the form\n",
    "        ``<component>__<parameter>`` so that it's possible to update each\n",
    "        component of a nested object.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "        if not params:\n",
    "            # Simple optimization to gain speed (inspect is slow)\n",
    "            return self\n",
    "        valid_params = self.get_params(deep=True)\n",
    "\n",
    "        nested_params = defaultdict(dict)  # grouped by prefix\n",
    "        for key, value in params.items():\n",
    "            key, delim, sub_key = key.partition('__')\n",
    "            if key not in valid_params:\n",
    "                raise ValueError('Invalid parameter %s for estimator %s. '\n",
    "                                 'Check the list of available parameters '\n",
    "                                 'with `estimator.get_params().keys()`.' %\n",
    "                                 (key, self))\n",
    "\n",
    "            if delim:\n",
    "                nested_params[key][sub_key] = value\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "                valid_params[key] = value\n",
    "\n",
    "        for key, sub_params in nested_params.items():\n",
    "            valid_params[key].set_params(**sub_params)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=1.796e-02, with an active set of 41 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 68 iterations, i.e. alpha=7.300e-03, with an active set of 68 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=3.629e-03, with an active set of 125 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 185 iterations, i.e. alpha=1.007e-03, with an active set of 184 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 260 iterations, i.e. alpha=4.189e+03, with an active set of 228 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.029e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.015e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 100 iterations, i.e. alpha=4.853e-03, with an active set of 100 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=2.763e-03, with an active set of 147 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=2.672e-03, with an active set of 147 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=2.335e-03, with an active set of 159 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 161 iterations, i.e. alpha=2.311e-03, with an active set of 159 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 334 iterations, i.e. alpha=1.811e+20, with an active set of 227 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 334 iterations, i.e. alpha=6.753e+19, with an active set of 227 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 334 iterations, i.e. alpha=5.640e+19, with an active set of 227 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 334 iterations, i.e. alpha=1.014e+19, with an active set of 227 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.648e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=7.239e-03, with an active set of 94 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 296 iterations, i.e. alpha=5.237e+05, with an active set of 224 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 296 iterations, i.e. alpha=4.382e+05, with an active set of 224 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 302 iterations, i.e. alpha=1.330e+09, with an active set of 228 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.915e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.553e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.767e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=7.767e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 98 iterations, i.e. alpha=5.177e-03, with an active set of 98 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=3.753e-03, with an active set of 127 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=3.171e-03, with an active set of 143 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=2.501e-03, with an active set of 158 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 173 iterations, i.e. alpha=1.855e-03, with an active set of 171 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 269 iterations, i.e. alpha=3.484e+05, with an active set of 212 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 284 iterations, i.e. alpha=1.640e+07, with an active set of 216 regressors, and the smallest cholesky pivot element being 8.941e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 297 iterations, i.e. alpha=1.420e+10, with an active set of 221 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 297 iterations, i.e. alpha=1.323e+10, with an active set of 221 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 297 iterations, i.e. alpha=1.270e+10, with an active set of 221 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 297 iterations, i.e. alpha=3.923e+09, with an active set of 221 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 297 iterations, i.e. alpha=2.320e+09, with an active set of 221 regressors, and the smallest cholesky pivot element being 9.657e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 297 iterations, i.e. alpha=1.467e+09, with an active set of 221 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 297 iterations, i.e. alpha=2.764e+08, with an active set of 221 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.332e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.166e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.166e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.732e-03, with an active set of 100 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=5.732e-03, with an active set of 100 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 321 iterations, i.e. alpha=3.282e+10, with an active set of 226 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 321 iterations, i.e. alpha=1.973e+10, with an active set of 226 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 323 iterations, i.e. alpha=1.099e+10, with an active set of 227 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 323 iterations, i.e. alpha=1.062e+10, with an active set of 227 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 323 iterations, i.e. alpha=9.998e+09, with an active set of 227 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 323 iterations, i.e. alpha=2.356e+09, with an active set of 227 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 324 iterations, i.e. alpha=2.049e+09, with an active set of 228 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 324 iterations, i.e. alpha=1.741e+09, with an active set of 228 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 324 iterations, i.e. alpha=7.105e+08, with an active set of 228 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 324 iterations, i.e. alpha=4.482e+08, with an active set of 228 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 325 iterations, i.e. alpha=8.653e+07, with an active set of 229 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 325 iterations, i.e. alpha=2.769e+07, with an active set of 229 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 325 iterations, i.e. alpha=1.582e+07, with an active set of 229 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 325 iterations, i.e. alpha=7.092e+06, with an active set of 229 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=2.461e-02, with an active set of 32 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.158e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=1.158e-02, with an active set of 56 regressors, and the smallest cholesky pivot element being 6.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 95 iterations, i.e. alpha=5.925e-03, with an active set of 93 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=4.652e-03, with an active set of 111 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 295 iterations, i.e. alpha=1.401e+02, with an active set of 227 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.006e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.003e-02, with an active set of 40 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.003e-02, with an active set of 40 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 56 iterations, i.e. alpha=9.903e-03, with an active set of 56 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=9.587e-03, with an active set of 57 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=4.719e-03, with an active set of 106 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 108 iterations, i.e. alpha=4.687e-03, with an active set of 107 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 310 iterations, i.e. alpha=1.135e+04, with an active set of 229 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.368e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.503e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.503e-02, with an active set of 38 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.968e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.968e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.228e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.013e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=9.393e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=9.393e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=5.795e-03, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=4.697e-03, with an active set of 105 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.553e-03, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=4.553e-03, with an active set of 106 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 124 iterations, i.e. alpha=3.786e-03, with an active set of 124 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=2.859e-03, with an active set of 147 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=2.351e-03, with an active set of 155 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=2.188e-03, with an active set of 159 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 173 iterations, i.e. alpha=2.042e-03, with an active set of 167 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 174 iterations, i.e. alpha=1.856e-03, with an active set of 168 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 306 iterations, i.e. alpha=2.589e+08, with an active set of 228 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 306 iterations, i.e. alpha=3.732e+07, with an active set of 228 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 306 iterations, i.e. alpha=2.064e+07, with an active set of 228 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.288e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.144e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 81 iterations, i.e. alpha=5.464e-03, with an active set of 81 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=3.168e-03, with an active set of 129 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 136 iterations, i.e. alpha=2.811e-03, with an active set of 136 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 145 iterations, i.e. alpha=2.482e-03, with an active set of 145 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 180 iterations, i.e. alpha=1.261e-03, with an active set of 178 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 183 iterations, i.e. alpha=1.186e-03, with an active set of 181 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 189 iterations, i.e. alpha=1.088e-03, with an active set of 186 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 255 iterations, i.e. alpha=8.746e+01, with an active set of 214 regressors, and the smallest cholesky pivot element being 7.885e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=3.750e+03, with an active set of 215 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=1.232e+03, with an active set of 215 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=6.834e+02, with an active set of 215 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=6.512e+02, with an active set of 215 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=5.421e+02, with an active set of 215 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=4.406e+02, with an active set of 215 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=3.060e+02, with an active set of 215 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=2.107e+02, with an active set of 215 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=1.843e+02, with an active set of 215 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 261 iterations, i.e. alpha=6.311e+01, with an active set of 215 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.691e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.243e-02, with an active set of 41 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.918e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 57 iterations, i.e. alpha=1.119e-02, with an active set of 57 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 61 iterations, i.e. alpha=9.504e-03, with an active set of 61 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 113 iterations, i.e. alpha=4.735e-03, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 304 iterations, i.e. alpha=5.154e+12, with an active set of 225 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 304 iterations, i.e. alpha=4.476e+12, with an active set of 225 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 304 iterations, i.e. alpha=3.767e+12, with an active set of 225 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 304 iterations, i.e. alpha=3.107e+12, with an active set of 225 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 304 iterations, i.e. alpha=9.142e+11, with an active set of 225 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 304 iterations, i.e. alpha=6.200e+11, with an active set of 225 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.928e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.928e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.928e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9763720604000963"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LarsCV\n",
    "from sklearn.datasets import make_regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.55, random_state=40)\n",
    "reg = LarsCV(cv=10).fit(X_train, y_train)\n",
    "reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6249172964810399"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=4.485e-02, with an active set of 79 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 82 iterations, i.e. alpha=4.403e-02, with an active set of 79 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 256 iterations, i.e. alpha=1.554e+05, with an active set of 202 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.714e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.357e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 91 iterations, i.e. alpha=6.716e-03, with an active set of 91 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 137 iterations, i.e. alpha=3.309e-03, with an active set of 137 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=2.038e-03, with an active set of 159 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 167 iterations, i.e. alpha=1.832e-03, with an active set of 162 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 171 iterations, i.e. alpha=1.699e-03, with an active set of 165 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 172 iterations, i.e. alpha=1.602e-03, with an active set of 166 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 180 iterations, i.e. alpha=1.542e-03, with an active set of 172 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 180 iterations, i.e. alpha=1.516e-03, with an active set of 172 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 186 iterations, i.e. alpha=1.332e-03, with an active set of 177 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 186 iterations, i.e. alpha=1.244e-03, with an active set of 177 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 194 iterations, i.e. alpha=1.343e-03, with an active set of 183 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 194 iterations, i.e. alpha=1.254e-03, with an active set of 183 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 199 iterations, i.e. alpha=9.954e-04, with an active set of 186 regressors, and the smallest cholesky pivot element being 7.146e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 199 iterations, i.e. alpha=9.298e-04, with an active set of 186 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 235 iterations, i.e. alpha=7.825e+04, with an active set of 196 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 237 iterations, i.e. alpha=2.394e+04, with an active set of 198 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 238 iterations, i.e. alpha=2.351e+04, with an active set of 199 regressors, and the smallest cholesky pivot element being 7.743e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 238 iterations, i.e. alpha=1.826e+04, with an active set of 199 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 240 iterations, i.e. alpha=1.744e+04, with an active set of 201 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 240 iterations, i.e. alpha=8.091e+03, with an active set of 201 regressors, and the smallest cholesky pivot element being 9.541e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.296e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.296e-02, with an active set of 29 regressors, and the smallest cholesky pivot element being 4.215e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.630e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.630e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 9.186e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 89 iterations, i.e. alpha=1.073e-02, with an active set of 84 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 233 iterations, i.e. alpha=5.879e-01, with an active set of 200 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 233 iterations, i.e. alpha=3.082e-01, with an active set of 200 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 233 iterations, i.e. alpha=2.239e-01, with an active set of 200 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.693e-02, with an active set of 26 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 31 iterations, i.e. alpha=2.767e-02, with an active set of 31 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.384e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=1.384e-02, with an active set of 44 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.147e-02, with an active set of 51 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=6.607e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 80 iterations, i.e. alpha=6.607e-03, with an active set of 80 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.018e-03, with an active set of 85 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=5.534e-03, with an active set of 92 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 258 iterations, i.e. alpha=2.896e+08, with an active set of 199 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 258 iterations, i.e. alpha=2.583e+08, with an active set of 199 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 258 iterations, i.e. alpha=6.100e+06, with an active set of 199 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 242 iterations, i.e. alpha=6.059e+05, with an active set of 199 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 242 iterations, i.e. alpha=1.932e+05, with an active set of 199 regressors, and the smallest cholesky pivot element being 6.829e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8001462145405883"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LarsCV\n",
    "from sklearn.datasets import make_regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.60, random_state=40)\n",
    "reg = LarsCV(cv=5).fit(X_train, y_train)\n",
    "reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5054615957705041"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9446084545984627"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LarsCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, random_state=40)\n",
    "reg1 = LarsCV(cv=10).fit(X, y)\n",
    "get_params(deep=True)\n",
    "reg1.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.33195725e-05,  0.00000000e+00,  8.22819775e-04,  9.59327004e-02,\n",
       "        0.00000000e+00,  2.33891944e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "       -2.85982521e+00,  2.26798127e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.41713819e+00,  0.00000000e+00,  1.92206091e+01,  3.25164361e+01,\n",
       "        0.00000000e+00, -8.48455117e+00, -6.84344826e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -4.10338360e+00, -5.30517122e+00,  0.00000000e+00,\n",
       "        1.32491455e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        5.40154135e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.04194868e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -9.66266892e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -2.08866016e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        4.91293898e+00,  0.00000000e+00,  1.23988012e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.09754368e+01,  7.77973979e+00,\n",
       "        1.44668795e+01,  1.28714090e+02,  1.36625903e+01,  4.06524807e+01,\n",
       "        5.20688193e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.85275799e-02,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.79519667e+00,  1.57040256e+00,  0.00000000e+00,\n",
       "        7.32325322e+00,  2.32664080e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  8.08916556e+00,  1.43482249e+01,\n",
       "        7.41989320e+00,  1.52057431e+01,  1.47649054e+01,  2.92585761e+01,\n",
       "        4.64052667e+01,  3.80995390e+00,  1.92769382e-01,  1.87516921e+01,\n",
       "        1.73281997e+00,  3.40463732e+00,  0.00000000e+00,  6.06294662e+00,\n",
       "        2.90019227e+00,  0.00000000e+00, -2.76080833e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.48960272e+01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.33979949e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.49777639e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.27895874e+00,  1.08364411e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.42455944e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.50187232e-02,\n",
       "       -1.26154507e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.92261946e+00,\n",
       "       -7.24097454e+00, -4.56800371e+00, -1.02974422e+00, -1.85561154e+00,\n",
       "       -1.63288007e+00, -4.10962694e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        5.73721865e-01,  1.02394882e+00,  2.29667574e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.22091682e+00,  0.00000000e+00,\n",
       "       -4.75979179e-01, -4.76141263e+13, -4.76141263e+13, -4.76141263e+13])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.36410795e-05,  0.00000000e+00,  8.41836289e-04,  9.58554218e-02,\n",
       "        0.00000000e+00,  2.34397043e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "       -2.78332194e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.27479044e+00,  0.00000000e+00,  1.89832683e+01,  3.23968973e+01,\n",
       "        0.00000000e+00, -8.49451997e+00, -6.80260877e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -4.09476233e+00, -5.24092331e+00,  0.00000000e+00,\n",
       "        1.30274926e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        5.14721820e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.02926071e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -8.97995208e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.47547664e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        4.75102940e+00,  0.00000000e+00,  1.21782114e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.07854650e+01,  7.59807435e+00,\n",
       "        1.43057956e+01,  1.28518652e+02,  1.35082234e+01,  4.04746331e+01,\n",
       "        5.19179410e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.66613257e+00,  1.45732351e+00,  0.00000000e+00,\n",
       "        7.19812202e+00,  2.30324642e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  7.87410765e+00,  1.42123573e+01,\n",
       "        7.19588874e+00,  1.50789321e+01,  1.46554515e+01,  2.90951737e+01,\n",
       "        4.62177496e+01,  3.59672121e+00,  0.00000000e+00,  1.85748502e+01,\n",
       "        1.51435729e+00,  3.29594076e+00,  0.00000000e+00,  5.96470169e+00,\n",
       "        2.78452223e+00,  0.00000000e+00, -2.70339580e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.47212835e+01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.32114516e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.44136911e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.18011582e+00,  9.64887601e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  4.28571538e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  2.48297722e-02,\n",
       "       -1.25003003e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -1.82177185e+00,\n",
       "       -7.05317371e+00, -4.53176448e+00, -9.73368880e-01, -1.81681998e+00,\n",
       "       -1.60817053e+00, -3.82343675e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        5.75396719e-01,  1.01962469e+00,  2.30361391e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.21403984e+00,  0.00000000e+00,\n",
       "       -4.74980594e-01, -4.11661919e+13, -4.11661919e+13, -4.11661919e+13])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.302e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=9.084e-03, with an active set of 62 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=7.807e-03, with an active set of 65 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 69 iterations, i.e. alpha=6.909e-03, with an active set of 68 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=6.808e-03, with an active set of 70 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=5.920e-03, with an active set of 78 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 96 iterations, i.e. alpha=4.997e-03, with an active set of 95 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=4.395e-03, with an active set of 116 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=3.842e-03, with an active set of 132 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 140 iterations, i.e. alpha=3.671e-03, with an active set of 139 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=3.371e-03, with an active set of 148 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 149 iterations, i.e. alpha=3.357e-03, with an active set of 148 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=3.211e-03, with an active set of 157 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 164 iterations, i.e. alpha=3.051e-03, with an active set of 161 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 182 iterations, i.e. alpha=2.554e-03, with an active set of 178 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 191 iterations, i.e. alpha=2.505e-03, with an active set of 184 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 200 iterations, i.e. alpha=2.192e-03, with an active set of 192 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 201 iterations, i.e. alpha=2.163e-03, with an active set of 193 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 209 iterations, i.e. alpha=1.905e-03, with an active set of 201 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 213 iterations, i.e. alpha=1.885e-03, with an active set of 204 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 213 iterations, i.e. alpha=1.859e-03, with an active set of 204 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 220 iterations, i.e. alpha=3.033e-03, with an active set of 208 regressors, and the smallest cholesky pivot element being 9.064e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 227 iterations, i.e. alpha=2.007e-03, with an active set of 215 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 234 iterations, i.e. alpha=1.804e-03, with an active set of 220 regressors, and the smallest cholesky pivot element being 9.306e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 241 iterations, i.e. alpha=1.772e-03, with an active set of 225 regressors, and the smallest cholesky pivot element being 9.306e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 300 iterations, i.e. alpha=2.372e+00, with an active set of 254 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 325 iterations, i.e. alpha=1.027e+04, with an active set of 265 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.982e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.606e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.606e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.496e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=7.833e-03, with an active set of 74 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 75 iterations, i.e. alpha=7.833e-03, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 77 iterations, i.e. alpha=7.353e-03, with an active set of 76 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=5.115e-03, with an active set of 117 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=4.813e-03, with an active set of 138 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 337 iterations, i.e. alpha=3.826e+08, with an active set of 263 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 337 iterations, i.e. alpha=1.880e+08, with an active set of 263 regressors, and the smallest cholesky pivot element being 9.424e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 337 iterations, i.e. alpha=1.673e+08, with an active set of 263 regressors, and the smallest cholesky pivot element being 6.664e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 337 iterations, i.e. alpha=1.543e+08, with an active set of 263 regressors, and the smallest cholesky pivot element being 5.576e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 337 iterations, i.e. alpha=7.436e+07, with an active set of 263 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 337 iterations, i.e. alpha=1.784e+07, with an active set of 263 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 337 iterations, i.e. alpha=5.790e+06, with an active set of 263 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.621e-02, with an active set of 27 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.138e-02, with an active set of 33 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.323e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 311 iterations, i.e. alpha=1.512e+03, with an active set of 263 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 314 iterations, i.e. alpha=1.785e+03, with an active set of 265 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 315 iterations, i.e. alpha=6.419e+02, with an active set of 266 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 315 iterations, i.e. alpha=5.901e+02, with an active set of 266 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 315 iterations, i.e. alpha=2.191e+01, with an active set of 266 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.049e-02, with an active set of 17 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.025e-02, with an active set of 41 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 41 iterations, i.e. alpha=2.025e-02, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=1.342e-02, with an active set of 53 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.227e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.068e-02, with an active set of 59 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 63 iterations, i.e. alpha=9.141e-03, with an active set of 63 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.461e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=6.417e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=5.086e-03, with an active set of 104 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 110 iterations, i.e. alpha=4.747e-03, with an active set of 109 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 356 iterations, i.e. alpha=2.843e+05, with an active set of 273 regressors, and the smallest cholesky pivot element being 4.081e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 356 iterations, i.e. alpha=2.364e+05, with an active set of 273 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 356 iterations, i.e. alpha=1.403e+05, with an active set of 273 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 356 iterations, i.e. alpha=8.394e+04, with an active set of 273 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 356 iterations, i.e. alpha=5.773e+04, with an active set of 273 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 356 iterations, i.e. alpha=2.740e+03, with an active set of 273 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=3.362e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.473e-02, with an active set of 33 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 48 iterations, i.e. alpha=1.681e-02, with an active set of 48 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.169e-02, with an active set of 54 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 85 iterations, i.e. alpha=5.934e-03, with an active set of 84 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 273 iterations, i.e. alpha=9.141e-03, with an active set of 237 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 320 iterations, i.e. alpha=2.546e+03, with an active set of 265 regressors, and the smallest cholesky pivot element being 9.365e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 320 iterations, i.e. alpha=2.256e+02, with an active set of 265 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 320 iterations, i.e. alpha=1.904e+02, with an active set of 265 regressors, and the smallest cholesky pivot element being 8.495e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 320 iterations, i.e. alpha=1.852e+02, with an active set of 265 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 320 iterations, i.e. alpha=1.116e+02, with an active set of 265 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 320 iterations, i.e. alpha=1.091e+02, with an active set of 265 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 320 iterations, i.e. alpha=4.761e+01, with an active set of 265 regressors, and the smallest cholesky pivot element being 8.025e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=2.575e-02, with an active set of 28 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9134755837351"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LarsCV\n",
    "from sklearn.datasets import make_regression\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "reg = LarsCV(cv=5).fit(X_train, y_train)\n",
    "reg.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6202528941062789"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.35296305e-05, -2.26776352e-01,  5.79381106e-03,  1.21122074e-01,\n",
       "       -1.93985850e+00,  6.28342251e-02, -3.64575270e+00,  2.02893009e+00,\n",
       "       -7.12309369e+00,  5.72505270e-01, -6.81944191e-02,  4.10639970e+00,\n",
       "        1.40970944e+00, -4.16170384e+00,  1.67815398e+01,  2.22763231e+01,\n",
       "       -1.66643168e+01, -1.07791161e+01, -9.71245795e+00,  8.43931046e-02,\n",
       "       -4.62342385e-01, -4.06903814e+00, -1.21994143e+01, -6.70552716e+00,\n",
       "        1.44001081e+00, -2.14640775e+00, -3.03282743e+00, -7.88642499e-01,\n",
       "        8.78123459e+00, -7.41370775e-01,  3.23926335e+00,  3.87315802e+00,\n",
       "        4.71110912e+00,  3.79287085e+00,  2.30442018e+00, -3.11320295e+00,\n",
       "       -2.64300934e+00,  4.44417376e-01, -2.98292303e+00, -2.30902065e+00,\n",
       "       -2.07465514e+00, -3.86431154e+00, -5.80784155e+00, -1.85745800e+00,\n",
       "       -2.40487121e+00,  3.96297957e-01, -1.50380393e+00,  1.54861141e+00,\n",
       "        4.63238935e-01, -1.36377745e+01, -7.85347728e-01,  3.56123925e+00,\n",
       "       -2.46427268e+00,  7.23059276e-01, -3.88294116e+00,  1.95039960e+00,\n",
       "        1.32589026e+00, -2.15789579e-01, -1.73175796e+00, -2.94284223e-01,\n",
       "        7.92526121e-01,  2.26838851e+00, -2.14737624e+00,  3.29594657e+00,\n",
       "       -5.66439721e+00, -5.93441871e-01,  1.16475602e-01, -3.55059068e+00,\n",
       "       -3.77038926e+00, -2.12778735e+00, -1.27338897e+00, -3.50472159e-01,\n",
       "       -1.63254276e+00,  1.98717989e+00, -1.19445657e+00,  1.58179303e+00,\n",
       "       -3.23842656e+00, -2.87145504e-01,  1.82909884e+00, -8.25577209e-01,\n",
       "       -1.78570839e+00, -6.10734558e+00, -7.45978322e-01, -2.86402873e+00,\n",
       "       -6.81638783e-01,  1.18904752e+00, -5.00278067e+00, -8.11257268e+00,\n",
       "        1.50912015e+00,  1.11935693e+00,  2.00308348e+00, -1.83764628e-01,\n",
       "       -6.41321226e-01,  1.11642929e+00,  1.23366655e+00, -3.73438977e+00,\n",
       "        4.53396303e-01, -1.87475560e+00,  2.30271466e+00, -3.58064526e+00,\n",
       "        2.62943267e+00, -3.71429577e+00, -5.41497663e+00, -3.51215060e+00,\n",
       "       -3.93003559e+00, -4.02954268e+00, -4.45805847e+00, -1.79141179e+00,\n",
       "       -1.65646208e+00,  1.12100811e-01, -2.03389206e-01,  3.76011022e+00,\n",
       "        3.01555163e+00, -1.46999598e-01,  1.88155396e+00, -8.09852501e+00,\n",
       "        3.19364921e+00, -1.83531041e+00,  1.16262070e+01, -2.06940618e+00,\n",
       "       -6.32558034e+00, -4.39417097e+00, -4.38380044e+00, -5.75803423e+00,\n",
       "       -7.35706571e+00, -7.05091512e+00,  1.70097232e+00, -3.81108497e-01,\n",
       "        1.70846978e+01,  7.56439736e+01,  1.35803090e+01,  1.89545934e+01,\n",
       "        3.00986618e+01, -8.81742169e-01, -6.46041429e-01,  0.00000000e+00,\n",
       "        1.72569461e+00,  1.63915351e+00,  1.56609394e+00, -1.94141788e+00,\n",
       "       -1.78466832e+00, -6.94459504e+00,  1.25463949e+00,  4.42352794e-01,\n",
       "       -6.67217110e+00, -7.09608916e+00, -3.60271175e+00, -3.73053233e+00,\n",
       "        4.66955388e+00, -1.72400881e+00,  1.40000924e+00,  4.79309248e+00,\n",
       "        2.55153873e+00, -6.37171980e-01,  1.01062465e+00,  1.41795654e+00,\n",
       "       -1.36253974e+00,  1.41476887e+00,  9.15350356e-01,  5.98208331e-01,\n",
       "        2.49578683e+00,  3.07762551e+00,  2.67922530e+00, -3.66295207e-01,\n",
       "        3.04956500e-01,  3.95643577e+00, -1.75557305e-01, -5.67719839e-02,\n",
       "       -4.72033020e-01, -2.25644028e+00, -8.57563936e-01, -1.16794654e+00,\n",
       "       -1.71157205e+00,  4.05725608e-01,  3.84381499e+00,  5.56818167e+00,\n",
       "        3.60246539e+00,  1.48917348e+00, -6.41885128e-01,  6.08124216e+00,\n",
       "       -8.94420657e-01,  4.74353765e+00, -9.43885439e-02,  6.49616922e+00,\n",
       "        3.11292063e+00, -2.77161263e+00,  9.04623900e-01, -1.10472423e+00,\n",
       "        1.29669057e+00, -1.00557696e+00,  5.53132706e-01,  1.07166659e+00,\n",
       "        2.54851616e+00,  4.28239937e+00,  1.45041007e+00, -5.80014044e-01,\n",
       "        4.38229001e-02,  1.61949736e-01,  2.74333421e-01, -2.60430836e+00,\n",
       "        5.96744754e-01,  3.17578491e+00,  7.98350108e+00,  6.15504685e+00,\n",
       "        3.41506375e+00,  4.04166112e-01, -6.98654463e-01, -2.54225720e+00,\n",
       "        2.69607820e-01, -2.57355097e+00, -3.01169145e-01, -1.21084965e+00,\n",
       "        1.07316693e+01,  1.69827910e+01, -4.05966997e+00, -1.94326875e+00,\n",
       "       -1.15719399e+00, -1.03556152e+00,  8.58899701e+00,  1.31314905e+01,\n",
       "        5.74289633e+00,  1.54946696e+01,  1.10601909e+01,  1.21206943e+01,\n",
       "        2.37334580e+01,  3.24321891e+00, -7.84238879e+00,  5.65201741e+00,\n",
       "        4.94892779e+00,  3.76985125e+00, -1.46116003e+00,  8.12962324e+00,\n",
       "        2.50833222e+00, -3.64109359e+00, -6.03315105e+00,  1.29923392e+00,\n",
       "        3.52905910e-01,  2.78677036e+00,  7.40399870e-01, -1.05802134e+00,\n",
       "        1.19929203e+00, -4.82470883e-01, -1.82756735e+00, -3.41804404e+01,\n",
       "       -1.11329586e+00, -5.31341718e+00,  1.09351380e-01, -1.87313820e+00,\n",
       "        2.86335995e+00,  5.33783726e-01, -3.47721609e-01,  1.23623305e+00,\n",
       "        4.57060210e+00,  4.54951652e+00, -8.74848894e+00, -2.86848398e+00,\n",
       "       -3.12960265e+00, -3.34505239e+00, -3.76857094e+00, -2.76197363e+00,\n",
       "       -8.49596042e-01, -2.61708415e+00, -6.13899395e+00, -7.34876328e+00,\n",
       "       -1.86616286e+00, -5.72945634e-02, -1.08479283e+00, -5.55278926e+00,\n",
       "       -7.34743092e+00, -8.74255755e-01, -3.60011516e+00, -6.02277427e+00,\n",
       "        1.69832315e-01,  1.08130196e+00, -8.13092118e-01, -4.16235940e-01,\n",
       "       -6.07738420e-01, -3.34467308e+00,  1.21397057e-01, -7.46103728e-01,\n",
       "       -8.10805106e-01, -3.90415828e+00,  1.74284718e+00,  1.27326103e+00,\n",
       "        6.88982673e-01,  1.11750588e+00, -2.68132282e+00,  2.08807194e+00,\n",
       "       -2.21234063e+00, -2.13090454e+00, -1.34248859e+00, -3.51090903e+00,\n",
       "       -1.34389643e+00,  3.83373736e-01,  2.04816313e+00,  3.71606433e+00,\n",
       "       -2.42145708e-01,  3.03734322e+00, -2.21006584e+00,  7.73764713e-01,\n",
       "       -3.73129693e+00, -3.21523911e+00, -2.64981666e+00, -3.56294887e+00,\n",
       "       -1.28868613e+00, -2.63398000e+00, -3.13396120e+00, -1.22597952e+01,\n",
       "       -9.17539101e+00, -9.93677891e+00,  3.18290114e+00,  1.82774768e+00,\n",
       "       -3.22991863e+00, -5.46525173e+00,  3.56569695e+00, -1.10993006e+00,\n",
       "       -1.20865021e+00, -2.94612223e+00, -2.46479723e+00,  1.18148550e-01,\n",
       "       -3.01678633e+00, -3.97983020e+00, -6.51997994e+00, -3.00738487e+00,\n",
       "       -1.61362461e+00, -4.82802066e-01, -4.07676904e+00, -9.10175227e-01,\n",
       "       -5.24834164e-01,  9.22998157e-01, -5.45199754e-02,  9.45620061e-01,\n",
       "       -1.57856597e-01,  1.60561924e+00, -6.48542565e-01,  1.80506488e-01,\n",
       "       -2.34991886e+00, -4.41100774e-01,  5.22028992e-01,  2.30271466e+00,\n",
       "       -5.66439721e+00, -8.74848894e+00,  1.54861141e+00, -2.19253439e+00,\n",
       "       -8.19779662e+00, -3.48346868e+00, -3.21310075e-03, -4.08178410e-01,\n",
       "       -4.57836212e-01,  1.54420068e+00,  2.47213556e+00,  3.92034802e+00,\n",
       "        4.87696306e+00,  5.78230178e+00,  6.70863839e+00,  3.05314227e+00,\n",
       "        6.55276594e-01,  0.00000000e+00, -3.70841887e+00,  1.69502790e+00,\n",
       "       -1.69502790e+00, -6.15482056e-01, -1.61285034e+00,  2.22833239e+00])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.70, random_state=40)\n",
    "reg5 = linear_model.Ridge(alpha=.5)\n",
    "reg5.fit(X,y)\n",
    "reg5.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9827825342900034"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.60, random_state=40)\n",
    "regbr1 = linear_model.BayesianRidge()\n",
    "regbr1.fit(X_train, y_train)  \n",
    "regbr1.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798397793268874"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.70, random_state=40)\n",
    "regbr2 = linear_model.BayesianRidge()\n",
    "regbr1.fit(X_test, y_test)\n",
    "regbr1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.74124140e-05, -1.59555215e-02,  1.06292748e-02,  1.27078800e-01,\n",
       "       -2.60658622e+00,  4.34588154e-02, -4.76609842e+00,  6.55473007e-09,\n",
       "       -6.41307183e+00, -5.41162085e+00,  2.96215964e+00,  1.04194926e+01,\n",
       "        2.39191421e+00, -1.48299882e+00,  2.08714408e+01,  2.22180758e+01,\n",
       "       -2.79487389e+01, -1.22915746e+01, -1.10485319e+01, -3.52163787e-01,\n",
       "       -3.87741804e-09, -4.46204287e+00, -1.85187768e+01, -3.22772852e-09,\n",
       "       -3.00335062e+00, -8.73850158e-10, -3.80273648e+00, -1.77070171e+00,\n",
       "       -7.18700470e+00, -6.05557007e-01,  5.03226676e+00,  5.98476904e+00,\n",
       "       -4.12280757e-09,  4.89704835e+00,  4.58020748e+00,  1.99696155e-09,\n",
       "       -2.11635119e+00,  5.82811384e-01, -2.22468999e+00, -2.00280904e+00,\n",
       "        6.87156543e-09, -5.35587016e+00, -5.44511951e+00, -4.78418403e+00,\n",
       "       -1.68217888e+00,  1.60376452e+00, -4.84302102e-09,  1.47358702e+00,\n",
       "       -5.72466479e-01,  4.14647684e-09, -2.07391042e+00,  1.88759291e+00,\n",
       "       -2.60449526e+00, -3.08502931e-01, -5.57514111e+00, -8.91744880e-10,\n",
       "        3.86120133e+00, -5.41833392e-01, -1.21094440e+00, -2.64523261e-01,\n",
       "        1.01816308e+00,  2.95652093e+00,  2.67150687e-09,  2.81132818e+00,\n",
       "        6.72908612e-09, -8.24649448e-01,  2.41347822e-01, -1.67450085e-09,\n",
       "       -4.50334022e+00, -2.96291603e+00, -2.38253815e+00, -1.75674471e+00,\n",
       "       -1.68677627e+00,  1.70240198e+00, -1.30047442e+00,  2.38580191e+00,\n",
       "       -4.02305666e+00, -2.41010965e-09,  2.76903235e+00,  6.13188038e-01,\n",
       "       -3.05023508e+00, -3.59510482e+00, -2.76271522e+00, -2.75524129e+00,\n",
       "       -2.47675113e+00,  1.82887710e+00, -4.20656343e+00, -1.02822215e+01,\n",
       "        2.15097792e+00, -1.79815364e-09,  4.60504740e+00,  1.18876346e+00,\n",
       "        1.03683247e+00,  1.93174469e-01,  4.88597716e-01, -3.85986147e+00,\n",
       "       -3.99267333e-01, -2.11051328e+00,  8.67040551e-01, -6.97630491e+00,\n",
       "        3.49266878e+00, -3.38297870e+00, -6.84710305e+00, -5.35198474e+00,\n",
       "       -3.38144895e+00, -5.73618701e+00, -5.42625315e+00, -4.56693440e+00,\n",
       "       -3.80140688e+00, -2.44703332e-02, -7.97754210e-01,  3.73957743e+00,\n",
       "       -2.72058133e-09, -5.03136610e-02,  1.41713220e+00, -1.39177661e+01,\n",
       "        3.28421957e+00, -4.82057205e-09,  1.56804125e+01, -2.73374625e+00,\n",
       "       -7.99405179e+00, -4.96343674e+00, -6.44532727e+00,  4.00362330e-09,\n",
       "       -8.33257023e+00, -8.08203074e+00, -1.19624945e-09, -3.79791208e+00,\n",
       "        2.34222151e+01,  9.65563539e+01, -3.67156368e-09,  2.14492129e+01,\n",
       "        3.15011839e+01, -3.33331562e+00, -2.08349293e+00, -8.53091005e-10,\n",
       "        1.23604476e+00,  1.73817102e+00,  3.27324675e+00,  3.76071669e-01,\n",
       "        6.15268369e-01, -1.02598086e+01,  2.63055185e+00,  1.35826487e+00,\n",
       "       -7.31858953e+00, -8.19120568e+00, -5.63872524e+00, -3.46135940e+00,\n",
       "        4.44879517e+00, -3.54059580e+00,  1.45494182e+00,  3.96675431e+00,\n",
       "        1.85673445e+00, -8.83689140e-01,  2.03086841e+00,  6.40871875e-01,\n",
       "       -1.97906376e+00,  1.61918270e+00,  4.16349208e-01, -1.62533645e-01,\n",
       "        3.67463910e+00,  8.03251737e-10,  3.47601762e+00,  6.09690834e-02,\n",
       "        7.03680585e-01,  3.04827232e+00,  1.50905710e-01, -2.34104966e+00,\n",
       "       -2.77846388e+00, -1.23809894e+00, -1.34875786e+00,  8.87346302e-01,\n",
       "       -2.00767509e+00, -1.16131789e-01,  4.65980569e+00,  4.18353502e+00,\n",
       "        1.12761584e-09,  2.06201772e+00, -6.48953126e-01,  8.06352043e+00,\n",
       "        2.78688341e-01,  8.11733756e+00,  1.34781074e-01,  8.93116516e+00,\n",
       "        6.13148311e+00, -4.98066876e-09, -5.53458929e-12,  1.27411035e-09,\n",
       "       -3.32492967e-09,  1.24851418e+00,  1.96268726e+00, -4.12210285e-09,\n",
       "        2.84839675e+00,  4.11050153e+00,  1.52044109e+00,  7.31855102e-01,\n",
       "        1.74763663e+00,  1.97877367e+00,  1.94031697e+00, -1.26666006e-01,\n",
       "        2.46216255e+00,  2.76312835e+00,  7.68382394e+00,  6.62366684e+00,\n",
       "        2.46254614e+00,  1.00685113e+00, -8.65586099e-01, -3.54277705e+00,\n",
       "       -4.52702325e-10,  1.70291424e+00, -6.70106233e-01, -3.39637786e+00,\n",
       "        1.17963258e+01, -3.25792812e-09, -1.20841376e+01, -4.80267581e+00,\n",
       "        6.04933434e-10, -1.71491702e+00,  1.17173846e+01,  1.35935397e+01,\n",
       "        1.69023419e+00,  1.68024660e+01,  8.65642968e+00,  1.32461724e+01,\n",
       "        3.03248101e+01,  4.94756536e+00, -1.49230621e+01,  8.96250751e+00,\n",
       "        3.94198789e+00,  5.86122509e+00, -5.89719026e+00,  1.03655185e+01,\n",
       "        1.03340451e-01, -5.00835877e+00, -6.40104811e+00,  1.18605403e+00,\n",
       "       -7.91318997e-01,  4.04704327e+00, -1.91374561e-09, -2.12868896e+00,\n",
       "        2.05982419e+00,  1.18770025e+00, -1.82057120e+00, -3.19764504e-09,\n",
       "        1.09864802e-10, -3.48530763e+00,  1.30262383e+00, -1.00532074e+00,\n",
       "        2.34141694e+00,  8.43129730e-01,  1.98064210e-01,  3.35747478e+00,\n",
       "        4.59257738e+00,  5.67885578e+00, -1.26444699e+01,  4.64283363e-10,\n",
       "       -2.61910004e+00, -2.13209163e-09, -5.80748195e+00, -4.74712335e+00,\n",
       "        4.63018346e-10,  1.10301167e-09, -8.54615822e+00, -8.06763766e+00,\n",
       "       -1.61483001e-09, -6.06862185e-13, -3.45979114e-09, -6.22826155e+00,\n",
       "       -8.05001897e+00, -1.16325116e+00, -3.11653775e+00, -8.51559109e+00,\n",
       "       -9.46001585e-01, -1.99446469e-10,  1.20801554e-09, -2.35720356e+00,\n",
       "        1.22489376e-09, -4.50749230e+00,  1.11098685e-10,  7.29006846e-01,\n",
       "        7.64985287e-01, -2.31347165e+00,  2.26852752e+00,  8.93152208e-01,\n",
       "       -1.68661202e-01,  1.41195118e+00, -8.90033304e+00,  4.27120952e+00,\n",
       "       -5.14795447e+00, -3.81893239e+00, -5.32218027e+00, -5.59229305e+00,\n",
       "        8.98586692e-03,  1.60349014e+00,  3.22206153e+00,  4.12433977e+00,\n",
       "       -6.10597035e-01,  6.33198359e+00, -3.46869519e+00, -1.02140896e+00,\n",
       "       -5.71696120e+00, -4.50351902e+00, -2.65217455e+00, -4.05024127e+00,\n",
       "       -1.05202755e+00, -1.97968022e+00, -9.51712767e+00, -1.37030202e+01,\n",
       "       -1.01999650e+01, -1.11581085e+01,  4.77626652e+00,  3.06985541e+00,\n",
       "        1.61125192e-09, -5.28264765e+00,  5.05246768e+00, -1.47692960e+00,\n",
       "       -1.06189838e+00, -1.29172533e+00, -2.54832365e+00, -1.35598066e-01,\n",
       "       -2.83846857e+00, -4.39861959e+00, -5.14512937e+00, -2.79567530e+00,\n",
       "       -4.34781784e+00, -2.13785402e+00, -6.51877619e+00, -9.53750266e-01,\n",
       "       -9.90775533e-01,  9.95646736e-01, -4.19930243e-01,  8.53237353e-01,\n",
       "       -1.19074649e+00,  2.13747761e+00,  3.06540105e-01,  2.67154586e-01,\n",
       "       -1.69687758e+00, -5.66208443e-01,  3.04481870e-01,  8.67040566e-01,\n",
       "       -8.31615449e-11, -1.26444699e+01,  1.47358703e+00, -1.85931596e+00,\n",
       "       -3.97798674e+00, -4.37443697e+00, -4.85021439e-01, -3.06116480e-01,\n",
       "       -1.21420241e+00,  9.35310438e-01,  1.74287459e+00,  3.67097201e+00,\n",
       "        4.52075648e+00,  5.27431048e+00,  6.37669834e+00,  3.17585999e+00,\n",
       "       -1.28564054e+00,  7.98390345e-25, -1.89021945e+00,  1.32293027e+00,\n",
       "       -1.32293026e+00, -1.67304558e+00, -2.39877507e+00,  4.07182066e+00])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regbr1.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8512323851148572"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80, random_state=40)\n",
    "regbr2 = linear_model.BayesianRidge()\n",
    "regbr2.fit(X_train, y_train)  \n",
    "regbr2.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784798991291271"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regbr2.fit(X_test,y_test)\n",
    "regbr2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoLarsCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.356e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 33 iterations, i.e. alpha=2.795e-02, with an active set of 33 regressors, and the smallest cholesky pivot element being 5.373e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 132 iterations, alpha=4.933e-03, previous alpha=4.910e-03, with an active set of 131 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.970e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 50 iterations, alpha=2.115e-02, previous alpha=2.115e-02, with an active set of 47 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 32 iterations, i.e. alpha=3.215e-02, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 52 iterations, i.e. alpha=1.607e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=4.748e-03, with an active set of 121 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 130 iterations, i.e. alpha=4.745e-03, with an active set of 122 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 142 iterations, i.e. alpha=4.114e-03, with an active set of 134 regressors, and the smallest cholesky pivot element being 6.409e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 162 iterations, alpha=3.392e-03, previous alpha=3.391e-03, with an active set of 153 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.394e-02, with an active set of 18 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 37 iterations, i.e. alpha=2.527e-02, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=3.616e-03, with an active set of 169 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 172 iterations, alpha=3.572e-03, previous alpha=3.486e-03, with an active set of 171 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 24 iterations, i.e. alpha=4.158e-02, with an active set of 24 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 214 iterations, i.e. alpha=2.191e-03, with an active set of 198 regressors, and the smallest cholesky pivot element being 5.053e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 214 iterations, i.e. alpha=2.191e-03, with an active set of 198 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 214 iterations, i.e. alpha=2.191e-03, with an active set of 198 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 220 iterations, alpha=2.075e-03, previous alpha=2.033e-03, with an active set of 203 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.480e-02, with an active set of 50 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.893e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=3.246e-03, with an active set of 143 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=3.246e-03, with an active set of 143 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 152 iterations, i.e. alpha=3.096e-03, with an active set of 152 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 182 iterations, i.e. alpha=2.160e-03, with an active set of 182 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 205 iterations, i.e. alpha=1.611e-03, with an active set of 203 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 205 iterations, i.e. alpha=1.611e-03, with an active set of 203 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 205 iterations, i.e. alpha=1.611e-03, with an active set of 203 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 205 iterations, i.e. alpha=1.611e-03, with an active set of 203 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 208 iterations, i.e. alpha=1.525e-03, with an active set of 206 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 221 iterations, i.e. alpha=1.048e-03, with an active set of 219 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 233 iterations, i.e. alpha=8.988e-04, with an active set of 231 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 236 iterations, alpha=8.620e-04, previous alpha=8.593e-04, with an active set of 233 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=4.002e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 40 iterations, i.e. alpha=2.060e-02, with an active set of 40 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=1.755e-02, with an active set of 45 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 67 iterations, i.e. alpha=8.686e-03, with an active set of 67 regressors, and the smallest cholesky pivot element being 8.229e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 109 iterations, i.e. alpha=4.257e-03, with an active set of 109 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=3.830e-03, with an active set of 117 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 180 iterations, i.e. alpha=1.914e-03, with an active set of 180 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 189 iterations, alpha=1.780e-03, previous alpha=1.780e-03, with an active set of 188 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.440e-02, with an active set of 20 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.170e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=2.170e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.076e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 8.429e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 58 iterations, i.e. alpha=1.076e-02, with an active set of 58 regressors, and the smallest cholesky pivot element being 8.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 111 iterations, i.e. alpha=5.346e-03, with an active set of 111 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 175 iterations, i.e. alpha=2.664e-03, with an active set of 175 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 175 iterations, i.e. alpha=2.664e-03, with an active set of 175 regressors, and the smallest cholesky pivot element being 5.674e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 181 iterations, alpha=2.509e-03, previous alpha=2.475e-03, with an active set of 180 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.483e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 4.593e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.852e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.852e-02, with an active set of 47 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=9.223e-03, with an active set of 65 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 65 iterations, i.e. alpha=9.223e-03, with an active set of 65 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 79 iterations, alpha=8.186e-03, previous alpha=7.880e-03, with an active set of 78 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.952e-02, with an active set of 22 regressors, and the smallest cholesky pivot element being 4.344e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 155 iterations, i.e. alpha=3.589e-03, with an active set of 151 regressors, and the smallest cholesky pivot element being 5.268e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Land Rover Range Rover 3.0 Diesel LWB Vogue', 131.16074424205706), (u'Land Rover Range Rover Sport SE', 68.3461597509184), (u'Mercedes-Benz GLS 350d 4MATIC', 31.59993284320368), (u'Audi Q7 45 TDI Quattro Technology', 27.79569422734697), (u'Mercedes-Benz E-Class E 350 d', 23.56840987278091), (u'BMW X3 xDrive 20d Luxury Line', 19.477251659428788), (u'Audi Q7 45 TDI Quattro Premium Plus', 19.43880602740982), (u'BMW 7 Series 730Ld Eminence', 16.055682309709432), (u'Mercedes-Benz GLC 220d 4MATIC Style', 13.96422343929832), (u'Land Rover Discovery HSE Luxury 3.0 TD6', 13.461942718231123), (u'Mercedes-Benz GLE 250d', 13.024488301352664), (u'Mercedes-Benz GLE 350d', 12.57883627611335), (u'Jaguar XF 2.0 Diesel Portfolio', 11.54111304812165), (u'Land Rover Discovery SE 3.0 TD6', 10.60459189742351), (u'Mercedes-Benz GLC 43 AMG Coupe', 6.896776833871253), (u'Land Rover Discovery Sport TD4 HSE 7S', 6.040116350538806), (u'Jaguar XE 2.0L Diesel Prestige', 4.877693529942559), (u'Mini Cooper 5 DOOR D', 4.734638968229489), (u'Mercedes-Benz New C-Class Progressive C 200', 4.706023227606287), (u'Mercedes-Benz E-Class E 220 d', 4.466602699256548), (u'Toyota Fortuner 2.8 2WD MT', 4.334478705089415), (u'Mercedes-Benz S-Class S 350 d', 3.20103609127274), (u'Mini Cooper 3 DOOR D', 3.149865660056962), (u'Mercedes-Benz C-Class Progressive C 220d', 2.7762824246619435), (u'Mini Cooper Convertible S', 2.38884047944513), (u'Toyota Fortuner 2.8 2WD AT', 2.030689036061759), (u'Mini Clubman Cooper S', 1.4841242301362567), (u'Audi Q3 30 TDI Premium FWD', 1.4763698317928453), (2019L, 1.1326316415221187), (2017L, 0.9543327369899388), (2018L, 0.9107183094000342), (u'BMW 7 Series 730Ld Design Pure Excellence CBU', 0.780861246257003), (u'Coimbatore', 0.3777076895202201), (u'New_Price', 0.2763978990631109), (u'Diesel', 0.21745180881927698), (u'Power', 0.08644649763013543), (u'Engine', 0.0012534946348781019), (u'Mileage', 0.0), (u'Seats', 0.0), (u'Audi A3 35 TDI Premium Plus', 0.0), (u'Audi A4 30 TFSI Premium Plus', 0.0), (u'Audi A4 35 TDI Technology', 0.0), (u'Audi A6 35 TDI Matrix', 0.0), (u'Audi A6 35 TFSI Matrix', 0.0), (u'Audi Q3 35 TDI Quattro Premium Plus', 0.0), (u'Audi RS5 Coupe', 0.0), (u'BMW 3 Series Luxury Line', 0.0), (u'BMW 3 Series Sport', 0.0), (u'BMW X1 M Sport sDrive 20d', 0.0), (u'BMW X1 sDrive 20d xLine', 0.0), (u'BMW X1 xDrive 20d M Sport', 0.0), (u'BMW X3 xDrive 20d xLine', 0.0), (u'Datsun GO Plus T Petrol', 0.0), (u'Datsun redi-GO S', 0.0), (u'Datsun redi-GO T', 0.0), (u'Datsun redi-GO T Option', 0.0), (u'Fiat Linea Classic Plus 1.3 Multijet', 0.0), (u'Ford Aspire Ambiente Diesel', 0.0), (u'Ford Aspire Titanium Diesel', 0.0), (u'Ford Aspire Titanium Plus Diesel', 0.0), (u'Ford EcoSport 1.5 Diesel Titanium', 0.0), (u'Ford EcoSport 1.5 Diesel Titanium Plus', 0.0), (u'Ford EcoSport 1.5 Diesel Trend', 0.0), (u'Ford EcoSport 1.5 Petrol Titanium', 0.0), (u'Ford EcoSport 1.5 Petrol Titanium Plus AT', 0.0), (u'Ford EcoSport 1.5 Petrol Trend', 0.0), (u'Ford Ecosport Signature Edition Diesel', 0.0), (u'Ford Endeavour Titanium 4X2', 0.0), (u'Ford Figo Titanium Diesel', 0.0), (u'Ford Freestyle Titanium Petrol', 0.0), (u'Ford Freestyle Titanium Plus Petrol', 0.0), (u'Ford Mustang V8', 0.0), (u'Honda Amaze S Diesel', 0.0), (u'Honda Amaze S Petrol', 0.0), (u'Honda Amaze V CVT Petrol', 0.0), (u'Honda Amaze V Petrol', 0.0), (u'Honda Amaze VX Diesel', 0.0), (u'Honda Amaze VX Petrol', 0.0), (u'Honda BR-V i-DTEC VX MT', 0.0), (u'Honda BR-V i-VTEC S MT', 0.0), (u'Honda BRV i-VTEC V CVT', 0.0), (u'Honda BRV i-VTEC V MT', 0.0), (u'Honda Brio 1.2 S MT', 0.0), (u'Honda Brio 1.2 S Option MT', 0.0), (u'Honda Brio 1.2 VX AT', 0.0), (u'Honda Brio 1.2 VX MT', 0.0), (u'Honda CR-V Petrol 2WD', 0.0), (u'Honda City i-DTEC SV', 0.0), (u'Honda City i-DTEC ZX', 0.0), (u'Honda City i-VTEC CVT V', 0.0), (u'Honda City i-VTEC CVT VX', 0.0), (u'Honda City i-VTEC CVT ZX', 0.0), (u'Honda City i-VTEC V', 0.0), (u'Honda City i-VTEC VX', 0.0), (u'Honda Jazz Exclusive CVT', 0.0), (u'Honda Jazz V', 0.0), (u'Honda Jazz V CVT', 0.0), (u'Honda Jazz VX', 0.0), (u'Honda Jazz VX Diesel', 0.0), (u'Honda WR-V Edge Edition i-VTEC S', 0.0), (u'Honda WRV i-VTEC VX', 0.0), (u'Hyundai Creta 1.4 E Plus Diesel', 0.0), (u'Hyundai Creta 1.6 SX', 0.0), (u'Hyundai Creta 1.6 SX Automatic Diesel', 0.0), (u'Hyundai Creta 1.6 SX Option', 0.0), (u'Hyundai Creta 1.6 SX Option Diesel', 0.0), (u'Hyundai Creta 1.6 SX Option Executive', 0.0), (u'Hyundai Elantra 1.6 SX', 0.0), (u'Hyundai Elantra 1.6 SX Option AT', 0.0), (u'Hyundai Elantra 2.0 SX Option AT', 0.0), (u'Hyundai Elite i20 Asta Option', 0.0), (u'Hyundai Elite i20 Sportz Plus', 0.0), (u'Hyundai Grand i10 1.2 CRDi Asta', 0.0), (u'Hyundai Grand i10 1.2 CRDi Magna', 0.0), (u'Hyundai Grand i10 1.2 CRDi Sportz', 0.0), (u'Hyundai Grand i10 1.2 Kappa Asta', 0.0), (u'Hyundai Grand i10 1.2 Kappa Magna', 0.0), (u'Hyundai Grand i10 1.2 Kappa Magna AT', 0.0), (u'Hyundai Grand i10 1.2 Kappa Sportz', 0.0), (u'Hyundai Grand i10 1.2 Kappa Sportz AT', 0.0), (u'Hyundai Santro D Lite', 0.0), (u'Hyundai Tucson 2.0 Dual VTVT 2WD AT GL', 0.0), (u'Hyundai Verna CRDi 1.4 E', 0.0), (u'Hyundai Verna CRDi 1.6 AT SX Option', 0.0), (u'Hyundai Verna CRDi 1.6 AT SX Plus', 0.0), (u'Hyundai Verna CRDi 1.6 SX Option', 0.0), (u'Hyundai Verna VTVT 1.6 AT SX Option', 0.0), (u'Hyundai Verna VTVT 1.6 AT SX Plus', 0.0), (u'Hyundai Verna VTVT 1.6 SX', 0.0), (u'Hyundai Verna VTVT 1.6 SX Option', 0.0), (u'Hyundai Xcent 1.2 CRDi S', 0.0), (u'Hyundai Xcent 1.2 VTVT E', 0.0), (u'Hyundai Xcent 1.2 VTVT S', 0.0), (u'Hyundai Xcent 1.2 VTVT SX', 0.0), (u'Hyundai i20 Active SX Dual Tone Petrol', 0.0), (u'Hyundai i20 Active SX Petrol', 0.0), (u'Isuzu MUX 4WD', 0.0), (u'Jaguar XE Portfolio', 0.0), (u'Jeep Compass 1.4 Limited Option', 0.0), (u'Jeep Compass 2.0 Limited', 0.0), (u'Jeep Compass 2.0 Limited 4X4', 0.0), (u'Jeep Compass 2.0 Limited Option 4X4', 0.0), (u'Jeep Compass 2.0 Limited Option Black', 0.0), (u'Jeep Compass 2.0 Longitude', 0.0), (u'Jeep Compass 2.0 Sport', 0.0), (u'Land Rover Range Rover Evoque 2.0 TD4 HSE Dynamic', 0.0), (u'Land Rover Range Rover Sport HSE', 0.0), (u'Mahindra Bolero SLE', 0.0), (u'Mahindra Bolero ZLX', 0.0), (u'Mahindra E Verito D4', 0.0), (u'Mahindra KUV 100 G80 K6 Plus 5Str', 0.0), (u'Mahindra KUV 100 G80 K8', 0.0), (u'Mahindra KUV 100 G80 K8 Dual Tone', 0.0), (u'Mahindra NuvoSport N6', 0.0), (u'Mahindra NuvoSport N8', 0.0), (u'Mahindra Thar CRDe', 0.0), (u'Mahindra Verito 1.5 D4 BSIV', 0.0), (u'Mahindra XUV300 W8 Diesel', 0.0), (u'Mahindra XUV500 W7', 0.0), (u'Mahindra XUV500 W9 AT', 0.0), (u'Mahindra Xylo D4', 0.0), (u'Mahindra Xylo H4', 0.0), (u'Maruti Alto K10 LXI', 0.0), (u'Maruti Alto K10 LXI CNG Optional', 0.0), (u'Maruti Alto K10 LXI Optional', 0.0), (u'Maruti Alto K10 VXI', 0.0), (u'Maruti Baleno Alpha', 0.0), (u'Maruti Baleno Alpha CVT', 0.0), (u'Maruti Baleno Alpha Diesel', 0.0), (u'Maruti Baleno Delta', 0.0), (u'Maruti Baleno Delta CVT', 0.0), (u'Maruti Baleno RS Petrol', 0.0), (u'Maruti Baleno Zeta', 0.0), (u'Maruti Baleno Zeta CVT', 0.0), (u'Maruti Celerio CNG VXI MT', 0.0), (u'Maruti Celerio LXI MT', 0.0), (u'Maruti Celerio VXI AMT', 0.0), (u'Maruti Celerio VXI Optional AMT', 0.0), (u'Maruti Celerio ZXI AMT', 0.0), (u'Maruti Celerio ZXI MT', 0.0), (u'Maruti Celerio ZXI Optional AMT', 0.0), (u'Maruti Ciaz Alpha', 0.0), (u'Maruti Ciaz Zeta', 0.0), (u'Maruti Dzire AMT VDI', 0.0), (u'Maruti Dzire AMT VXI', 0.0), (u'Maruti Dzire AMT ZDI Plus', 0.0), (u'Maruti Dzire AMT ZXI Plus', 0.0), (u'Maruti Dzire LDI', 0.0), (u'Maruti Dzire VDI', 0.0), (u'Maruti Dzire VXI', 0.0), (u'Maruti Dzire ZDI', 0.0), (u'Maruti Dzire ZDI Plus', 0.0), (u'Maruti Eeco 5 Seater AC', 0.0), (u'Maruti Eeco 7 Seater Standard', 0.0), (u'Maruti Eeco CNG 5 Seater AC', 0.0), (u'Maruti Ertiga VDI', 0.0), (u'Maruti Ertiga VXI AT Petrol', 0.0), (u'Maruti Ertiga ZDI', 0.0), (u'Maruti Ertiga ZDI Plus', 0.0), (u'Maruti Ignis 1.2 AMT Alpha', 0.0), (u'Maruti Ignis 1.2 Alpha', 0.0), (u'Maruti Ignis 1.2 Delta', 0.0), (u'Maruti S-Cross Alpha DDiS 200 SH', 0.0), (u'Maruti S-Cross Delta DDiS 200 SH', 0.0), (u'Maruti S-Cross Zeta DDiS 200 SH', 0.0), (u'Maruti Swift LDI', 0.0), (u'Maruti Swift VDI', 0.0), (u'Maruti Swift VXI', 0.0), (u'Maruti Swift ZXI Plus', 0.0), (u'Maruti Vitara Brezza LDi', 0.0), (u'Maruti Vitara Brezza VDi', 0.0), (u'Maruti Vitara Brezza ZDi', 0.0), (u'Maruti Vitara Brezza ZDi Plus', 0.0), (u'Maruti Vitara Brezza ZDi Plus AMT Dual Tone', 0.0), (u'Maruti Vitara Brezza ZDi Plus Dual Tone', 0.0), (u'Maruti Wagon R CNG LXI', 0.0), (u'Maruti Wagon R LXI', 0.0), (u'Maruti Wagon R VXI', 0.0), (u'Maruti Wagon R VXI 1.2', 0.0), (u'Maruti Wagon R VXI AMT', 0.0), (u'Maruti Wagon R VXI AMT1.2', 0.0), (u'Maruti Wagon R ZXI AMT 1.2', 0.0), (u'Mercedes-Benz A Class A180 Sport', 0.0), (u'Mercedes-Benz CLA 200 CDI Sport', 0.0), (u'Mercedes-Benz CLA 200 CGI Sport', 0.0), (u'Mercedes-Benz E-Class E400 Cabriolet', 0.0), (u'Mercedes-Benz GLA Class 200 Sport', 0.0), (u'Mercedes-Benz GLA Class 200 d Sport', 0.0), (u'Mercedes-Benz GLA Class 200 d Style', 0.0), (u'Mercedes-Benz GLC 220d 4MATIC Sport', 0.0), (u'Mercedes-Benz GLS 350d Grand Edition', 0.0), (u'Mercedes-Benz SLC 43 AMG', 0.0), (u'Mini Cooper 3 DOOR S', 0.0), (u'Mitsubishi Pajero Sport 4X2 AT DualTone BlackTop', 0.0), (u'Nissan Micra Active XV', 0.0), (u'Nissan Micra XV CVT', 0.0), (u'Nissan Micra XV D', 0.0), (u'Nissan Sunny XE P', 0.0), (u'Nissan Sunny XV CVT', 0.0), (u'Nissan Sunny XV D', 0.0), (u'Nissan Terrano XL D Option', 0.0), (u'Nissan Terrano XV D Pre', 0.0), (u'Renault Duster 110PS Diesel RxZ', 0.0), (u'Renault Duster 110PS Diesel RxZ AMT', 0.0), (u'Renault Duster 110PS Diesel RxZ AWD', 0.0), (u'Renault Duster 85PS Diesel RxE', 0.0), (u'Renault KWID 1.0 RXT Optional', 0.0), (u'Renault KWID 1.0 RXT Optional AMT', 0.0), (u'Renault KWID Climber 1.0 AMT', 0.0), (u'Renault KWID Climber 1.0 MT', 0.0), (u'Renault KWID RXL', 0.0), (u'Renault KWID RXT Optional', 0.0), (u'Skoda Rapid 1.5 TDI AT Style', 0.0), (u'Skoda Rapid 1.5 TDI Ambition', 0.0), (u'Skoda Rapid 1.5 TDI Style', 0.0), (u'Skoda Rapid 1.6 MPI AT Ambition', 0.0), (u'Skoda Rapid 1.6 MPI Ambition', 0.0), (u'Skoda Superb L&K 1.8 TSI AT', 0.0), (u'Skoda Superb L&K 2.0 TDI AT', 0.0), (u'Tata Bolt Quadrajet XE', 0.0), (u'Tata Bolt Quadrajet XM', 0.0), (u'Tata Bolt Revotron XT', 0.0), (u'Tata Hexa XT', 0.0), (u'Tata Hexa XTA', 0.0), (u'Tata Nexon 1.2 Revotron XZ Plus', 0.0), (u'Tata Nexon 1.5 Revotorq XZ Plus Dual Tone', 0.0), (u'Tata Safari Storme VX Varicor 400', 0.0), (u'Tata Tiago 1.2 Revotron XM Option', 0.0), (u'Tata Tiago 1.2 Revotron XT', 0.0), (u'Tata Tiago 1.2 Revotron XT Option', 0.0), (u'Tata Tiago 1.2 Revotron XZ', 0.0), (u'Tata Tiago 1.2 Revotron XZ WO Alloy', 0.0), (u'Tata Tiago AMT 1.2 Revotron XZA', 0.0), (u'Tata Tigor XE Diesel', 0.0), (u'Tata Zest Quadrajet 1.3 75PS XE', 0.0), (u'Tata Zest Quadrajet 1.3 75PS XM', 0.0), (u'Tata Zest Quadrajet 1.3 XT', 0.0), (u'Tata Zest Revotron 1.2 XT', 0.0), (u'Tata Zest Revotron 1.2T XE', 0.0), (u'Tata Zest Revotron 1.2T XM', 0.0), (u'Tata Zest Revotron 1.2T XMS', 0.0), (u'Toyota Camry Hybrid 2.5', 0.0), (u'Toyota Corolla Altis 1.4 DG', 0.0), (u'Toyota Corolla Altis 1.8 G', 0.0), (u'Toyota Corolla Altis 1.8 G CVT', 0.0), (u'Toyota Corolla Altis 1.8 GL', 0.0), (u'Toyota Corolla Altis 1.8 VL CVT', 0.0), (u'Toyota Etios 1.4 VXD', 0.0), (u'Toyota Etios Cross 1.4L GD', 0.0), (u'Toyota Etios Cross 1.4L VD', 0.0), (u'Toyota Etios Liva 1.2 G', 0.0), (u'Toyota Etios Liva 1.2 VX Dual Tone', 0.0), (u'Toyota Etios Liva 1.4 GD', 0.0), (u'Toyota Fortuner 2.8 4WD MT', 0.0), (u'Toyota Innova Crysta 2.4 GX MT', 0.0), (u'Toyota Innova Crysta 2.4 GX MT 8S', 0.0), (u'Toyota Innova Crysta 2.4 VX MT', 0.0), (u'Toyota Innova Crysta 2.4 VX MT 8S', 0.0), (u'Toyota Innova Crysta 2.4 ZX MT', 0.0), (u'Toyota Innova Crysta 2.7 GX MT', 0.0), (u'Toyota Innova Crysta 2.8 GX AT', 0.0), (u'Toyota Innova Crysta 2.8 GX AT 8S', 0.0), (u'Toyota Innova Crysta 2.8 ZX AT', 0.0), (u'Toyota Platinum Etios 1.4 GXD', 0.0), (u'Volkswagen Ameo 1.2 MPI Highline', 0.0), (u'Volkswagen Ameo 1.5 TDI Comfortline', 0.0), (u'Volkswagen Ameo 1.5 TDI Highline', 0.0), (u'Volkswagen Polo 1.0 MPI Comfortline', 0.0), (u'Volkswagen Polo 1.0 MPI Trendline', 0.0), (u'Volkswagen Polo 1.5 TDI Comfortline', 0.0), (u'Volkswagen Polo 1.5 TDI Highline', 0.0), (u'Volkswagen Polo GT TSI', 0.0), (u'Volkswagen Tiguan 2.0 TDI Highline', 0.0), (u'Volkswagen Vento 1.5 TDI Comfortline', 0.0), (u'Volkswagen Vento 1.5 TDI Highline', 0.0), (u'Volkswagen Vento 1.5 TDI Highline AT', 0.0), (u'Volkswagen Vento 1.5 TDI Trendline', 0.0), (u'Volkswagen Vento 1.6 Comfortline', 0.0), (u'Volkswagen Vento 1.6 Highline', 0.0), (u'Volvo S60 D4 Momentum', 0.0), (u'Volvo V40 D3 R Design', 0.0), (u'Ahmedabad', 0.0), (u'Bangalore', 0.0), (u'Chennai', 0.0), (u'Delhi', 0.0), (u'Hyderabad', 0.0), (u'Jaipur', 0.0), (u'Kochi', 0.0), (u'Mumbai', 0.0), (u'Pune', 0.0), (2001L, 0.0), (2004L, 0.0), (2005L, 0.0), (2007L, 0.0), (2008L, 0.0), (2009L, 0.0), (2015L, 0.0), (2016L, 0.0), (u'CNG', 0.0), (u'Electric', 0.0), (u'Automatic', 0.0), (u'First', 0.0), (u'Third', 0.0), (u'Kilometers_Driven', -4.1650864013900617e-05), (2014L, -0.057052215862754385), (u'Manual', -0.36105396866661627), (u'Petrol', -0.38397590722753416), (u'Second', -0.44604680043213896), (u'Kolkata', -0.4907780647899427), (u'Hyundai Verna CRDi 1.6 SX', -0.9872056487185215), (2012L, -1.338674577841506), (2011L, -1.7198952606708726), (2013L, -1.7405724901684998), (u'Skoda Superb Style 1.8 TSI AT', -1.7963028207895662), (u'Skoda Superb Style 2.0 TDI AT', -2.5598850620273548), (u'Mitsubishi Pajero Sport 4X4', -3.8545249948854106), (u'Audi A4 35 TDI Premium Plus', -4.229925109084239), (u'BMW 5 Series 520d Luxury Line', -5.348207891150912), (2010L, -5.740330041984579), (u'BMW 3 Series 320d Luxury Line', -5.987593417242804), (u'BMW 5 Series 530d M Sport', -6.713737810463398), (u'BMW 3 Series 320d', -8.6867366507739), (u'Skoda Octavia RS', -14.174608577450003), (u'Porsche Cayenne Base', -22.47703262106389)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 224 iterations, i.e. alpha=1.794e-03, with an active set of 204 regressors, and the smallest cholesky pivot element being 4.942e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 224 iterations, i.e. alpha=1.794e-03, with an active set of 204 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 224 iterations, i.e. alpha=1.794e-03, with an active set of 204 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 225 iterations, i.e. alpha=1.725e-03, with an active set of 205 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 228 iterations, i.e. alpha=1.631e-03, with an active set of 208 regressors, and the smallest cholesky pivot element being 5.162e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:337: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 237 iterations, alpha=1.555e-03, previous alpha=1.535e-03, with an active set of 216 regressors.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.577e-02, with an active set of 25 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.940e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "C:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:311: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.940e-02, with an active set of 46 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoLarsCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "\n",
    "print(\"LassoLarsCV\")\n",
    "\n",
    "# specify the lasso regression model\n",
    "model=LassoLarsCV(cv=10, precompute=False).fit(X_train,y_train)\n",
    "\n",
    "# print variable names and regression coefficients\n",
    "print(sorted(zip(X.columns, model.coef_), key=lambda t : t[1],reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.16508640e-05,  0.00000000e+00,  1.25349463e-03,  8.64464976e-02,\n",
       "        0.00000000e+00,  2.76397899e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "       -4.22992511e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.47636983e+00,  0.00000000e+00,  1.94388060e+01,  2.77956942e+01,\n",
       "        0.00000000e+00, -8.68673665e+00, -5.98759342e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -5.34820789e+00, -6.71373781e+00,  7.80861246e-01,\n",
       "        1.60556823e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.94772517e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -9.87205649e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        4.87769353e+00,  0.00000000e+00,  1.15411130e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  1.34619427e+01,  1.06045919e+01,\n",
       "        6.04011635e+00,  1.31160744e+02,  0.00000000e+00,  0.00000000e+00,\n",
       "        6.83461598e+01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  2.77628242e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        4.46660270e+00,  2.35684099e+01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.39642234e+01,\n",
       "        6.89677683e+00,  1.30244883e+01,  1.25788363e+01,  3.15999328e+01,\n",
       "        0.00000000e+00,  4.70602323e+00,  3.20103609e+00,  0.00000000e+00,\n",
       "        1.48412423e+00,  3.14986566e+00,  0.00000000e+00,  4.73463897e+00,\n",
       "        2.38884048e+00,  0.00000000e+00, -3.85452499e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -2.24770326e+01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.41746086e+01,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00, -1.79630282e+00, -2.55988506e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  2.03068904e+00,  4.33447871e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  3.77707690e-01,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -4.90778065e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -5.74033004e+00, -1.71989526e+00, -1.33867458e+00,\n",
       "       -1.74057249e+00, -5.70522159e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        9.54332737e-01,  9.10718309e-01,  1.13263164e+00,  0.00000000e+00,\n",
       "        2.17451809e-01,  0.00000000e+00, -3.83975907e-01,  0.00000000e+00,\n",
       "       -3.61053969e-01,  0.00000000e+00, -4.46046800e-01,  0.00000000e+00])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[u'Kilometers_Driven', u'Mileage', u'Engine', u'Power',\n",
       "         u'Seats', u'New_Price', u'Audi A3 35 TDI Premium Plus',\n",
       "         u'Audi A4 30 TFSI Premium Plus',\n",
       "         u'Audi A4 35 TDI Premium Plus', u'Audi A4 35 TDI Technology',\n",
       "         u'Audi A6 35 TDI Matrix', u'Audi A6 35 TFSI Matrix',\n",
       "         u'Audi Q3 30 TDI Premium FWD',\n",
       "         u'Audi Q3 35 TDI Quattro Premium Plus',\n",
       "         u'Audi Q7 45 TDI Quattro Premium Plus',\n",
       "         u'Audi Q7 45 TDI Quattro Technology', u'Audi RS5 Coupe',\n",
       "         u'BMW 3 Series 320d', u'BMW 3 Series 320d Luxury Line',\n",
       "         u'BMW 3 Series Luxury Line', u'BMW 3 Series Sport',\n",
       "         u'BMW 5 Series 520d Luxury Line', u'BMW 5 Series 530d M Sport',\n",
       "         u'BMW 7 Series 730Ld Design Pure Excellence CBU',\n",
       "         u'BMW 7 Series 730Ld Eminence', u'BMW X1 M Sport sDrive 20d',\n",
       "         u'BMW X1 sDrive 20d xLine', u'BMW X1 xDrive 20d M Sport',\n",
       "         u'BMW X3 xDrive 20d Luxury Line', u'BMW X3 xDrive 20d xLine',\n",
       "         u'Datsun GO Plus T Petrol', u'Datsun redi-GO S',\n",
       "         u'Datsun redi-GO T', u'Datsun redi-GO T Option',\n",
       "         u'Fiat Linea Classic Plus 1.3 Multijet',\n",
       "         u'Ford Aspire Ambiente Diesel', u'Ford Aspire Titanium Diesel',\n",
       "         u'Ford Aspire Titanium Plus Diesel',\n",
       "         u'Ford EcoSport 1.5 Diesel Titanium',\n",
       "         u'Ford EcoSport 1.5 Diesel Titanium Plus',\n",
       "         u'Ford EcoSport 1.5 Diesel Trend',\n",
       "         u'Ford EcoSport 1.5 Petrol Titanium',\n",
       "         u'Ford EcoSport 1.5 Petrol Titanium Plus AT',\n",
       "         u'Ford EcoSport 1.5 Petrol Trend',\n",
       "         u'Ford Ecosport Signature Edition Diesel',\n",
       "         u'Ford Endeavour Titanium 4X2', u'Ford Figo Titanium Diesel',\n",
       "         u'Ford Freestyle Titanium Petrol',\n",
       "         u'Ford Freestyle Titanium Plus Petrol', u'Ford Mustang V8',\n",
       "         u'Honda Amaze S Diesel', u'Honda Amaze S Petrol',\n",
       "         u'Honda Amaze V CVT Petrol', u'Honda Amaze V Petrol',\n",
       "         u'Honda Amaze VX Diesel', u'Honda Amaze VX Petrol',\n",
       "         u'Honda BR-V i-DTEC VX MT', u'Honda BR-V i-VTEC S MT',\n",
       "         u'Honda BRV i-VTEC V CVT', u'Honda BRV i-VTEC V MT',\n",
       "         u'Honda Brio 1.2 S MT', u'Honda Brio 1.2 S Option MT',\n",
       "         u'Honda Brio 1.2 VX AT', u'Honda Brio 1.2 VX MT',\n",
       "         u'Honda CR-V Petrol 2WD', u'Honda City i-DTEC SV',\n",
       "         u'Honda City i-DTEC ZX', u'Honda City i-VTEC CVT V',\n",
       "         u'Honda City i-VTEC CVT VX', u'Honda City i-VTEC CVT ZX',\n",
       "         u'Honda City i-VTEC V', u'Honda City i-VTEC VX',\n",
       "         u'Honda Jazz Exclusive CVT', u'Honda Jazz V',\n",
       "         u'Honda Jazz V CVT', u'Honda Jazz VX', u'Honda Jazz VX Diesel',\n",
       "         u'Honda WR-V Edge Edition i-VTEC S', u'Honda WRV i-VTEC VX',\n",
       "         u'Hyundai Creta 1.4 E Plus Diesel', u'Hyundai Creta 1.6 SX',\n",
       "         u'Hyundai Creta 1.6 SX Automatic Diesel',\n",
       "         u'Hyundai Creta 1.6 SX Option',\n",
       "         u'Hyundai Creta 1.6 SX Option Diesel',\n",
       "         u'Hyundai Creta 1.6 SX Option Executive',\n",
       "         u'Hyundai Elantra 1.6 SX', u'Hyundai Elantra 1.6 SX Option AT',\n",
       "         u'Hyundai Elantra 2.0 SX Option AT',\n",
       "         u'Hyundai Elite i20 Asta Option',\n",
       "         u'Hyundai Elite i20 Sportz Plus',\n",
       "         u'Hyundai Grand i10 1.2 CRDi Asta',\n",
       "         u'Hyundai Grand i10 1.2 CRDi Magna',\n",
       "         u'Hyundai Grand i10 1.2 CRDi Sportz',\n",
       "         u'Hyundai Grand i10 1.2 Kappa Asta',\n",
       "         u'Hyundai Grand i10 1.2 Kappa Magna',\n",
       "         u'Hyundai Grand i10 1.2 Kappa Magna AT',\n",
       "         u'Hyundai Grand i10 1.2 Kappa Sportz',\n",
       "         u'Hyundai Grand i10 1.2 Kappa Sportz AT',\n",
       "         u'Hyundai Santro D Lite',\n",
       "         u'Hyundai Tucson 2.0 Dual VTVT 2WD AT GL',\n",
       "         u'Hyundai Verna CRDi 1.4 E',\n",
       "         u'Hyundai Verna CRDi 1.6 AT SX Option',\n",
       "         u'Hyundai Verna CRDi 1.6 AT SX Plus',\n",
       "         u'Hyundai Verna CRDi 1.6 SX',\n",
       "         u'Hyundai Verna CRDi 1.6 SX Option',\n",
       "         u'Hyundai Verna VTVT 1.6 AT SX Option',\n",
       "         u'Hyundai Verna VTVT 1.6 AT SX Plus',\n",
       "         u'Hyundai Verna VTVT 1.6 SX',\n",
       "         u'Hyundai Verna VTVT 1.6 SX Option',\n",
       "         u'Hyundai Xcent 1.2 CRDi S', u'Hyundai Xcent 1.2 VTVT E',\n",
       "         u'Hyundai Xcent 1.2 VTVT S', u'Hyundai Xcent 1.2 VTVT SX',\n",
       "         u'Hyundai i20 Active SX Dual Tone Petrol',\n",
       "         u'Hyundai i20 Active SX Petrol', u'Isuzu MUX 4WD',\n",
       "         u'Jaguar XE 2.0L Diesel Prestige', u'Jaguar XE Portfolio',\n",
       "         u'Jaguar XF 2.0 Diesel Portfolio',\n",
       "         u'Jeep Compass 1.4 Limited Option',\n",
       "         u'Jeep Compass 2.0 Limited', u'Jeep Compass 2.0 Limited 4X4',\n",
       "         u'Jeep Compass 2.0 Limited Option 4X4',\n",
       "         u'Jeep Compass 2.0 Limited Option Black',\n",
       "         u'Jeep Compass 2.0 Longitude', u'Jeep Compass 2.0 Sport',\n",
       "         u'Land Rover Discovery HSE Luxury 3.0 TD6',\n",
       "         u'Land Rover Discovery SE 3.0 TD6',\n",
       "         u'Land Rover Discovery Sport TD4 HSE 7S',\n",
       "         u'Land Rover Range Rover 3.0 Diesel LWB Vogue',\n",
       "         u'Land Rover Range Rover Evoque 2.0 TD4 HSE Dynamic',\n",
       "         u'Land Rover Range Rover Sport HSE',\n",
       "         u'Land Rover Range Rover Sport SE', u'Mahindra Bolero SLE',\n",
       "         u'Mahindra Bolero ZLX', u'Mahindra E Verito D4',\n",
       "         u'Mahindra KUV 100 G80 K6 Plus 5Str',\n",
       "         u'Mahindra KUV 100 G80 K8',\n",
       "         u'Mahindra KUV 100 G80 K8 Dual Tone', u'Mahindra NuvoSport N6',\n",
       "         u'Mahindra NuvoSport N8', u'Mahindra Thar CRDe',\n",
       "         u'Mahindra Verito 1.5 D4 BSIV', u'Mahindra XUV300 W8 Diesel',\n",
       "         u'Mahindra XUV500 W7', u'Mahindra XUV500 W9 AT',\n",
       "         u'Mahindra Xylo D4', u'Mahindra Xylo H4',\n",
       "         u'Maruti Alto K10 LXI', u'Maruti Alto K10 LXI CNG Optional',\n",
       "         u'Maruti Alto K10 LXI Optional', u'Maruti Alto K10 VXI',\n",
       "         u'Maruti Baleno Alpha', u'Maruti Baleno Alpha CVT',\n",
       "         u'Maruti Baleno Alpha Diesel', u'Maruti Baleno Delta',\n",
       "         u'Maruti Baleno Delta CVT', u'Maruti Baleno RS Petrol',\n",
       "         u'Maruti Baleno Zeta', u'Maruti Baleno Zeta CVT',\n",
       "         u'Maruti Celerio CNG VXI MT', u'Maruti Celerio LXI MT',\n",
       "         u'Maruti Celerio VXI AMT', u'Maruti Celerio VXI Optional AMT',\n",
       "         u'Maruti Celerio ZXI AMT', u'Maruti Celerio ZXI MT',\n",
       "         u'Maruti Celerio ZXI Optional AMT', u'Maruti Ciaz Alpha',\n",
       "         u'Maruti Ciaz Zeta', u'Maruti Dzire AMT VDI',\n",
       "         u'Maruti Dzire AMT VXI', u'Maruti Dzire AMT ZDI Plus',\n",
       "         u'Maruti Dzire AMT ZXI Plus', u'Maruti Dzire LDI',\n",
       "         u'Maruti Dzire VDI', u'Maruti Dzire VXI', u'Maruti Dzire ZDI',\n",
       "         u'Maruti Dzire ZDI Plus', u'Maruti Eeco 5 Seater AC',\n",
       "         u'Maruti Eeco 7 Seater Standard',\n",
       "         u'Maruti Eeco CNG 5 Seater AC', u'Maruti Ertiga VDI',\n",
       "         u'Maruti Ertiga VXI AT Petrol', u'Maruti Ertiga ZDI',\n",
       "         u'Maruti Ertiga ZDI Plus', u'Maruti Ignis 1.2 AMT Alpha',\n",
       "         u'Maruti Ignis 1.2 Alpha', u'Maruti Ignis 1.2 Delta',\n",
       "         u'Maruti S-Cross Alpha DDiS 200 SH',\n",
       "         u'Maruti S-Cross Delta DDiS 200 SH',\n",
       "         u'Maruti S-Cross Zeta DDiS 200 SH', u'Maruti Swift LDI',\n",
       "         u'Maruti Swift VDI', u'Maruti Swift VXI',\n",
       "         u'Maruti Swift ZXI Plus', u'Maruti Vitara Brezza LDi',\n",
       "         u'Maruti Vitara Brezza VDi', u'Maruti Vitara Brezza ZDi',\n",
       "         u'Maruti Vitara Brezza ZDi Plus',\n",
       "         u'Maruti Vitara Brezza ZDi Plus AMT Dual Tone',\n",
       "         u'Maruti Vitara Brezza ZDi Plus Dual Tone',\n",
       "         u'Maruti Wagon R CNG LXI', u'Maruti Wagon R LXI',\n",
       "         u'Maruti Wagon R VXI', u'Maruti Wagon R VXI 1.2',\n",
       "         u'Maruti Wagon R VXI AMT', u'Maruti Wagon R VXI AMT1.2',\n",
       "         u'Maruti Wagon R ZXI AMT 1.2',\n",
       "         u'Mercedes-Benz A Class A180 Sport',\n",
       "         u'Mercedes-Benz C-Class Progressive C 220d',\n",
       "         u'Mercedes-Benz CLA 200 CDI Sport',\n",
       "         u'Mercedes-Benz CLA 200 CGI Sport',\n",
       "         u'Mercedes-Benz E-Class E 220 d',\n",
       "         u'Mercedes-Benz E-Class E 350 d',\n",
       "         u'Mercedes-Benz E-Class E400 Cabriolet',\n",
       "         u'Mercedes-Benz GLA Class 200 Sport',\n",
       "         u'Mercedes-Benz GLA Class 200 d Sport',\n",
       "         u'Mercedes-Benz GLA Class 200 d Style',\n",
       "         u'Mercedes-Benz GLC 220d 4MATIC Sport',\n",
       "         u'Mercedes-Benz GLC 220d 4MATIC Style',\n",
       "         u'Mercedes-Benz GLC 43 AMG Coupe', u'Mercedes-Benz GLE 250d',\n",
       "         u'Mercedes-Benz GLE 350d', u'Mercedes-Benz GLS 350d 4MATIC',\n",
       "         u'Mercedes-Benz GLS 350d Grand Edition',\n",
       "         u'Mercedes-Benz New C-Class Progressive C 200',\n",
       "         u'Mercedes-Benz S-Class S 350 d', u'Mercedes-Benz SLC 43 AMG',\n",
       "         u'Mini Clubman Cooper S', u'Mini Cooper 3 DOOR D',\n",
       "         u'Mini Cooper 3 DOOR S', u'Mini Cooper 5 DOOR D',\n",
       "         u'Mini Cooper Convertible S',\n",
       "         u'Mitsubishi Pajero Sport 4X2 AT DualTone BlackTop',\n",
       "         u'Mitsubishi Pajero Sport 4X4', u'Nissan Micra Active XV',\n",
       "         u'Nissan Micra XV CVT', u'Nissan Micra XV D',\n",
       "         u'Nissan Sunny XE P', u'Nissan Sunny XV CVT',\n",
       "         u'Nissan Sunny XV D', u'Nissan Terrano XL D Option',\n",
       "         u'Nissan Terrano XV D Pre', u'Porsche Cayenne Base',\n",
       "         u'Renault Duster 110PS Diesel RxZ',\n",
       "         u'Renault Duster 110PS Diesel RxZ AMT',\n",
       "         u'Renault Duster 110PS Diesel RxZ AWD',\n",
       "         u'Renault Duster 85PS Diesel RxE',\n",
       "         u'Renault KWID 1.0 RXT Optional',\n",
       "         u'Renault KWID 1.0 RXT Optional AMT',\n",
       "         u'Renault KWID Climber 1.0 AMT',\n",
       "         u'Renault KWID Climber 1.0 MT', u'Renault KWID RXL',\n",
       "         u'Renault KWID RXT Optional', u'Skoda Octavia RS',\n",
       "         u'Skoda Rapid 1.5 TDI AT Style',\n",
       "         u'Skoda Rapid 1.5 TDI Ambition', u'Skoda Rapid 1.5 TDI Style',\n",
       "         u'Skoda Rapid 1.6 MPI AT Ambition',\n",
       "         u'Skoda Rapid 1.6 MPI Ambition',\n",
       "         u'Skoda Superb L&K 1.8 TSI AT', u'Skoda Superb L&K 2.0 TDI AT',\n",
       "         u'Skoda Superb Style 1.8 TSI AT',\n",
       "         u'Skoda Superb Style 2.0 TDI AT', u'Tata Bolt Quadrajet XE',\n",
       "         u'Tata Bolt Quadrajet XM', u'Tata Bolt Revotron XT',\n",
       "         u'Tata Hexa XT', u'Tata Hexa XTA',\n",
       "         u'Tata Nexon 1.2 Revotron XZ Plus',\n",
       "         u'Tata Nexon 1.5 Revotorq XZ Plus Dual Tone',\n",
       "         u'Tata Safari Storme VX Varicor 400',\n",
       "         u'Tata Tiago 1.2 Revotron XM Option',\n",
       "         u'Tata Tiago 1.2 Revotron XT',\n",
       "         u'Tata Tiago 1.2 Revotron XT Option',\n",
       "         u'Tata Tiago 1.2 Revotron XZ',\n",
       "         u'Tata Tiago 1.2 Revotron XZ WO Alloy',\n",
       "         u'Tata Tiago AMT 1.2 Revotron XZA', u'Tata Tigor XE Diesel',\n",
       "         u'Tata Zest Quadrajet 1.3 75PS XE',\n",
       "         u'Tata Zest Quadrajet 1.3 75PS XM',\n",
       "         u'Tata Zest Quadrajet 1.3 XT', u'Tata Zest Revotron 1.2 XT',\n",
       "         u'Tata Zest Revotron 1.2T XE', u'Tata Zest Revotron 1.2T XM',\n",
       "         u'Tata Zest Revotron 1.2T XMS', u'Toyota Camry Hybrid 2.5',\n",
       "         u'Toyota Corolla Altis 1.4 DG', u'Toyota Corolla Altis 1.8 G',\n",
       "         u'Toyota Corolla Altis 1.8 G CVT',\n",
       "         u'Toyota Corolla Altis 1.8 GL',\n",
       "         u'Toyota Corolla Altis 1.8 VL CVT', u'Toyota Etios 1.4 VXD',\n",
       "         u'Toyota Etios Cross 1.4L GD', u'Toyota Etios Cross 1.4L VD',\n",
       "         u'Toyota Etios Liva 1.2 G',\n",
       "         u'Toyota Etios Liva 1.2 VX Dual Tone',\n",
       "         u'Toyota Etios Liva 1.4 GD', u'Toyota Fortuner 2.8 2WD AT',\n",
       "         u'Toyota Fortuner 2.8 2WD MT', u'Toyota Fortuner 2.8 4WD MT',\n",
       "         u'Toyota Innova Crysta 2.4 GX MT',\n",
       "         u'Toyota Innova Crysta 2.4 GX MT 8S',\n",
       "         u'Toyota Innova Crysta 2.4 VX MT',\n",
       "         u'Toyota Innova Crysta 2.4 VX MT 8S',\n",
       "         u'Toyota Innova Crysta 2.4 ZX MT',\n",
       "         u'Toyota Innova Crysta 2.7 GX MT',\n",
       "         u'Toyota Innova Crysta 2.8 GX AT',\n",
       "         u'Toyota Innova Crysta 2.8 GX AT 8S',\n",
       "         u'Toyota Innova Crysta 2.8 ZX AT',\n",
       "         u'Toyota Platinum Etios 1.4 GXD',\n",
       "         u'Volkswagen Ameo 1.2 MPI Highline',\n",
       "         u'Volkswagen Ameo 1.5 TDI Comfortline',\n",
       "         u'Volkswagen Ameo 1.5 TDI Highline',\n",
       "         u'Volkswagen Polo 1.0 MPI Comfortline',\n",
       "         u'Volkswagen Polo 1.0 MPI Trendline',\n",
       "         u'Volkswagen Polo 1.5 TDI Comfortline',\n",
       "         u'Volkswagen Polo 1.5 TDI Highline', u'Volkswagen Polo GT TSI',\n",
       "         u'Volkswagen Tiguan 2.0 TDI Highline',\n",
       "         u'Volkswagen Vento 1.5 TDI Comfortline',\n",
       "         u'Volkswagen Vento 1.5 TDI Highline',\n",
       "         u'Volkswagen Vento 1.5 TDI Highline AT',\n",
       "         u'Volkswagen Vento 1.5 TDI Trendline',\n",
       "         u'Volkswagen Vento 1.6 Comfortline',\n",
       "         u'Volkswagen Vento 1.6 Highline', u'Volvo S60 D4 Momentum',\n",
       "         u'Volvo V40 D3 R Design', u'Ahmedabad', u'Bangalore',\n",
       "         u'Chennai', u'Coimbatore', u'Delhi', u'Hyderabad', u'Jaipur',\n",
       "         u'Kochi', u'Kolkata', u'Mumbai', u'Pune', 2001L, 2004L, 2005L,\n",
       "         2007L, 2008L, 2009L, 2010L, 2011L, 2012L, 2013L, 2014L, 2015L,\n",
       "         2016L, 2017L, 2018L, 2019L, u'CNG', u'Diesel', u'Electric',\n",
       "         u'Petrol', u'Automatic', u'Manual', u'First', u'Second',\n",
       "         u'Third'],\n",
       "        [-4.1650864013900617e-05, 0.0, 0.0012534946348781019,\n",
       "         0.08644649763013543, 0.0, 0.2763978990631109, 0.0, 0.0,\n",
       "         -4.229925109084239, 0.0, 0.0, 0.0, 1.4763698317928453, 0.0,\n",
       "         19.43880602740982, 27.79569422734697, 0.0, -8.6867366507739,\n",
       "         -5.987593417242804, 0.0, 0.0, -5.348207891150912,\n",
       "         -6.713737810463398, 0.780861246257003, 16.055682309709432, 0.0,\n",
       "         0.0, 0.0, 19.477251659428788, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.9872056487185215,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         4.877693529942559, 0.0, 11.54111304812165, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 13.461942718231123, 10.60459189742351,\n",
       "         6.040116350538806, 131.16074424205706, 0.0, 0.0,\n",
       "         68.3461597509184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.7762824246619435, 0.0,\n",
       "         0.0, 4.466602699256548, 23.56840987278091, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 13.96422343929832, 6.896776833871253, 13.024488301352664,\n",
       "         12.57883627611335, 31.59993284320368, 0.0, 4.706023227606287,\n",
       "         3.20103609127274, 0.0, 1.4841242301362567, 3.149865660056962,\n",
       "         0.0, 4.734638968229489, 2.38884047944513, 0.0,\n",
       "         -3.8545249948854106, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         -22.47703262106389, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, -14.174608577450003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, -1.7963028207895662, -2.5598850620273548, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.030689036061759,\n",
       "         4.334478705089415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n",
       "         0.3777076895202201, 0.0, 0.0, 0.0, 0.0, -0.4907780647899427,\n",
       "         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -5.740330041984579,\n",
       "         -1.7198952606708726, -1.338674577841506, -1.7405724901684998,\n",
       "         -0.057052215862754385, 0.0, 0.0, 0.9543327369899388,\n",
       "         0.9107183094000342, 1.1326316415221187, 0.0,\n",
       "         0.21745180881927698, 0.0, -0.38397590722753416, 0.0,\n",
       "         -0.36105396866661627, 0.0, -0.44604680043213896, 0.0]]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.84815154e-03 -1.18303200e-03  1.38540861e-02  4.41025272e-03\n",
      " -2.04234606e-04  3.98165343e-03  0.00000000e+00  5.65514665e-08\n",
      "  5.96690191e-08  7.32346024e-07  1.54178910e-07  1.13708787e-07\n",
      "  0.00000000e+00  7.18192511e-09  0.00000000e+00  1.04384945e-07\n",
      "  0.00000000e+00  0.00000000e+00 -1.74592824e-08  5.64003708e-07\n",
      " -8.57611052e-07  4.56211448e-07  0.00000000e+00  2.50895770e-08\n",
      "  0.00000000e+00  2.16562676e-08  3.22379302e-07  0.00000000e+00\n",
      "  5.00875262e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.29688045e-08  1.08844899e-07  0.00000000e+00  0.00000000e+00\n",
      " -2.98830740e-08  0.00000000e+00  5.21930010e-08  0.00000000e+00\n",
      "  0.00000000e+00 -1.26500566e-07 -9.34817429e-07  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.53910750e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -7.68861800e-09\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.11645134e-07\n",
      "  0.00000000e+00  2.83541003e-07  4.46589766e-09  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.58817446e-07  0.00000000e+00\n",
      "  1.27565126e-08  0.00000000e+00  0.00000000e+00 -5.77939771e-07\n",
      "  0.00000000e+00 -4.35150143e-08  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -1.79785160e-07\n",
      " -2.65072565e-08  0.00000000e+00 -4.60397536e-07  0.00000000e+00\n",
      " -5.33886208e-07 -3.03431581e-07  4.84740324e-08  4.21005702e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -9.60135194e-08\n",
      "  0.00000000e+00  2.31061138e-07  0.00000000e+00  0.00000000e+00\n",
      "  9.86996543e-09  3.30170537e-08 -5.75936250e-06  0.00000000e+00\n",
      " -5.89070241e-07  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      " -8.65871000e-07  0.00000000e+00 -7.17346037e-07 -1.83435682e-07\n",
      "  0.00000000e+00  5.89196547e-08  0.00000000e+00  0.00000000e+00\n",
      "  1.98944306e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.49863793e-05  7.12762212e-07  0.00000000e+00  0.00000000e+00\n",
      " -4.62471249e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.87710586e-07  3.13658950e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.42183352e-07  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.03212526e-09  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -5.44121473e-07  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -6.20777531e-09\n",
      " -7.59971287e-08  0.00000000e+00  0.00000000e+00 -8.08378722e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.18662002e-08  1.62112634e-07  3.57359781e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  9.33546359e-08  0.00000000e+00 -1.57290686e-09\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  1.97291933e-07\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  6.05414969e-08\n",
      "  0.00000000e+00 -6.57413610e-06 -5.44881101e-05 -7.53305985e-07\n",
      "  2.59292885e-08  0.00000000e+00  0.00000000e+00 -4.49434163e-07\n",
      "  4.97685415e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  8.94624121e-08 -1.05047403e-05  5.07628909e-08 -3.58040530e-06\n",
      "  0.00000000e+00  1.29445026e-07  5.35025587e-07  5.46222622e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.33034186e-07  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  3.01043173e-06  0.00000000e+00  0.00000000e+00\n",
      " -2.37164919e-07  0.00000000e+00  0.00000000e+00  2.35849450e-07\n",
      "  0.00000000e+00  0.00000000e+00  5.52332054e-07  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.40694363e-07  1.32849601e-06\n",
      "  0.00000000e+00  0.00000000e+00  7.41751369e-08  0.00000000e+00\n",
      " -5.84871426e-08  0.00000000e+00  7.02324612e-09  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -6.93090983e-08 -1.17553936e-06\n",
      "  6.42774127e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00 -4.92452530e-06\n",
      "  2.15925271e-07 -1.81447309e-05  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  4.53709103e-07 -2.39613037e-08  2.38980246e-08  0.00000000e+00\n",
      "  5.49085327e-08  0.00000000e+00 -9.85964525e-07  0.00000000e+00\n",
      " -3.63079233e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.62635828e-07 -2.85730120e-07  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.97707063e-08  0.00000000e+00  0.00000000e+00\n",
      "  2.17018841e-08  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.03184491e-07 -3.30374153e-06\n",
      "  0.00000000e+00  0.00000000e+00  2.83413281e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  9.91703370e-08  0.00000000e+00\n",
      "  8.57763173e-08  0.00000000e+00  0.00000000e+00 -5.40379941e-08\n",
      "  0.00000000e+00 -2.65264365e-08  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  2.26785680e-07  0.00000000e+00 -2.95233452e-07\n",
      "  9.14867326e-09  2.99776802e-08  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00 -1.15984078e-07  1.31809199e-07  0.00000000e+00\n",
      "  0.00000000e+00  4.92661424e-08  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  1.09458138e-07  0.00000000e+00  0.00000000e+00\n",
      "  5.25809225e-07  6.20845660e-07 -3.96457994e-06 -2.84082676e-06\n",
      "  1.50596867e-05 -2.08142494e-05 -1.84258570e-07 -3.29768037e-06\n",
      " -2.04919207e-05 -3.43276953e-06 -2.30536894e-06  0.00000000e+00\n",
      "  1.27565126e-08  0.00000000e+00  0.00000000e+00  1.30160930e-08\n",
      " -1.35294607e-07  5.60793064e-07  1.94918809e-07  3.92945477e-08\n",
      "  1.09753468e-07  7.99657512e-07 -5.01501887e-07  1.17301729e-06\n",
      " -5.40479825e-05  1.67178869e-05 -6.06162779e-06  1.41311226e-07\n",
      "  5.64672418e-05  0.00000000e+00 -9.77338656e-05  5.65700521e-05\n",
      " -9.76953647e-05 -4.03149939e-05 -8.23075215e-07  1.27565126e-08]\n",
      "[-4.11253126e-05]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "regrpar1 = PassiveAggressiveRegressor(max_iter=100, random_state=0,\n",
    "tol=1e-3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.80, random_state=40)\n",
    "regrpar1.fit(X_train, y_train)  \n",
    "print(regrpar1.coef_)\n",
    "\n",
    "print(regrpar1.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-177.61750088918637"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regrpar1.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PolynomialFeatures (prepreprocessing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_ = poly.fit_transform(X)\n",
    "X_test_ = poly.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.56151610e+00,  4.51565200e+00,  5.62061795e+00, ...,\n",
       "       -2.60600405e+00,  0.00000000e+00, -4.94473235e-06])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate\n",
    "lg = LinearRegression()\n",
    "\n",
    "# Fit\n",
    "lg.fit(X_, y)\n",
    "\n",
    "# Obtain coefficients\n",
    "lg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.2, gamma='scale',\n",
       "  kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "import numpy as np\n",
    "n_samples, n_features = 10, 5\n",
    "clf = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "clf.fit(X, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Name', u'Location', u'Year', u'Kilometers_Driven', u'Fuel_Type',\n",
       "       u'Transmission', u'Owner_Type', u'Mileage', u'Engine', u'Power',\n",
       "       u'Seats', u'New_Price', u'Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[  4.5   17.5    9.95  15.    18.55   9.9    6.98   8.63   8.85  10.95\n  16.5   35.67  10.5    6.92   4.5    3.95   3.9   19.25   3.91  20.75\n   6.55   7.5   10.95   2.6   54.    11.5   37.    26.5    6.5   33.5\n   7.5    8.5    6.5    9.     3.25  26.     3.7    2.65  35.67  19.64\n   3.87  18.65   2.6   61.29   3.75   5.25   4.85  20.25  12.69  57.\n   3.13  62.67   7.15  25.99   2.05   3.75  52.    27.3   35.     9.49\n   9.48  34.5   28.    25.5    8.5    6.9   70.99  24.5   28.5    7.7\n   7.1    8.92  20.37  36.5   51.43  56.     3.3    8.    38.99  29.4\n   6.75  10.5   17.    19.     6.     8.12  24.82  48.5    9.45  17.5\n   8.9   29.5   11.25  10.79   5.7    2.75   9.25  11.31   5.43  18.75\n   8.5    4.9    5.7    7.75   3.2   23.5   10.65   8.46   2.5   10.9\n   4.32  17.63  56.     8.5    2.3    6.45   7.5   18.6    5.9   19.92\n   3.25   5.     2.7    2.66   3.61   8.65  20.    59.72   6.5   12.49\n  18.39   6.21  39.5   17.5    4.45  11.65  16.5    5.39  13.28  17.75\n  25.    12.41   7.94  20.5    7.    10.49   3.95   6.5    7.25   6.01\n  12.     5.88  51.35  22.     5.    19.75  11.44   3.5   10.46   5.68\n   5.98   4.77   7.2    5.9   25.    14.    20.4    5.68  12.75  18.\n   6.5   40.     6.15   6.24   5.5   11.97  18.9    2.95   5.25  24.68\n  29.5    1.9    2.5   13.95   6.06   2.96  67.     3.6    7.55   6.17\n   2.5   11.5    6.2   15.49   4.    19.92   9.2    3.5    4.5   16.35\n  12.5    5.8    5.    17.49  97.07   9.15   7.7    6.69  10.5   22.55\n   9.25   7.29  23.5    3.1    7.23  41.5   24.9   72.94   5.5   30.29\n  38.     2.74   4.9    3.22  37.3   70.8    6.03   7.8   28.03   5.\n   8.2    2.75   4.74   4.25   9.29   5.23   6.01   5.     9.     7.27\n   5.1    7.25   5.25   8.9   17.45   6.22  64.87  17.25   5.53  10.9\n   2.9   19.65  79.     5.75   7.27  30.     5.75   8.09  22.24   8.99\n   8.2    5.25   9.3   23.5    4.25  17.02   7.12  11.25   3.67  28.5\n  16.5    6.76   5.5    3.75  17.11   4.3    6.36  43.6    3.05  19.05\n  34.     4.     8.41   5.89   4.     3.94   5.25  83.96  21.     4.3\n  56.73   6.     7.53   5.8   31.     4.85   5.65  11.    11.6    3.95\n  41.6   23.     8.    17.75   3.6    1.75   5.99   6.71  14.21   5.11\n  22.99   3.6   13.7    3.1   12.9   17.5    8.45  10.75   5.22   9.18\n  13.9    3.25  78.8    9.69   5.65  22.5   28.9   22.    21.5    3.9\n   8.11   7.65   6.     7.25   9.     5.92   6.29  17.99   4.85   4.\n   3.6   21.75  45.     3.19   6.2   10.25  30.     8.99   4.5    9.38\n  17.85   3.75   8.95  56.5   21.    36.75  27.87   8.75  11.92   7.57\n   3.48  21.69  20.14   4.15   3.15   6.97  35.    11.5    6.5    8.\n   5.99   3.63   4.25   3.36  10.3    3.41  27.5    3.5    4.58  10.34\n   5.47  35.98   7.9    9.51  19.5    9.13   2.25   2.35  17.92  59.29\n   6.75   8.25   6.07   4.28  64.95   5.55  17.98   4.5    7.75  33.13\n   4.8    6.     9.92  24.     9.6   25.    10.44   9.9   20.     6.5\n   5.85  23.91   7.36  17.65   6.    18.25   4.25  16.9    5.     5.49\n   3.25   9.25   4.5    2.8   18.5    5.95   5.36   3.5    6.67   5.85\n  16.95   4.68   3.46  13.08   4.     9.51   3.25   2.02   3.95   6.84\n  21.73  31.55  27.36   4.65   2.25   3.72   3.45   6.08  18.    24.77\n  13.    26.5    8.95   7.7    8.5    2.5    4.65  17.5    7.34   5.9\n  27.    48.    59.65   7.29  23.8    5.65  25.75   3.85   3.5   11.15\n   8.9    8.61   9.25   9.25  10.15   3.75   8.25   4.4    4.5   10.3\n   5.5   21.26  11.5   12.5   12.46   5.1    6.3    5.8   20.    12.\n   5.95  22.2   30.77   6.25  39.84   5.74  35.     8.9   19.46   9.22\n   3.5   10.64   6.95   6.99  14.25   4.3    5.5    8.27  16.5   50.97\n  29.95   9.99   9.25   5.45   5.25  14.2    4.87  11.39   8.    70.66\n   3.25   9.18   4.11  19.97   4.5    5.85   2.15  41.5   11.     8.7\n   9.9    5.77  11.5    4.95  16.45   8.7   20.75   8.25  19.    17.55\n   3.     8.25   4.69   4.95   3.2   24.5   11.25  40.5   22.45 160.\n   8.61  26.11   6.5   16.81   3.41   2.75  35.82   8.02   2.85   6.67\n  20.5   11.31   3.6   11.45   6.37   8.2    5.49   7.7    7.75   2.93\n  32.73  21.66   2.99   3.15   3.31  17.85  19.6    4.45   8.85   7.56\n  18.    29.     5.87   7.75  21.42   4.1   16.5    5.     6.7    2.45\n   4.8    5.66  56.8   19.4    5.99   3.3    4.5   15.5    3.4    7.97\n   8.55   3.75  16.42  65.     3.95  10.5   17.27  27.     6.8    3.16\n  11.5    3.     9.75  36.64   5.43   3.     8.9    3.59   7.64   7.64\n  28.85   5.75  19.5    5.    11.88   4.     2.9    5.85  42.     6.28\n  30.    10.8   24.    37.92   7.25   9.66   2.25   3.46  13.95   2.8\n  43.     0.7    6.95   9.62   3.01  10.19   8.4    8.85  10.    46.95\n  13.25   9.7    8.37  24.37   6.     8.3    5.     2.9    8.86   7.9\n  16.27  48.    18.9    4.91  32.57  68.     8.4    8.39  56.14  15.\n   9.05   3.75   9.88   8.78   5.5    2.1    5.5    5.34  49.     3.75\n  60.75   2.65   5.27   7.4   27.    10.77  11.75   3.8    4.52   6.26\n   5.25   4.99   4.41   8.14   8.45  12.5    7.69  12.13   8.75   7.23\n   7.5    8.9    4.9    3.69   3.98   6.69   9.5    5.63  11.78   4.8\n   3.49   6.3    8.15  35.     4.5    6.9   35.5    6.28   3.7    2.9\n   4.25  29.5   10.9    4.3    8.6    6.75   7.4    7.21   4.2   16.5\n   5.15   3.7    2.25   8.49   5.5   11.99  29.99   4.25   8.25   8.2\n  24.46   3.9    3.65   5.     3.85  11.5    4.5   13.82   6.     2.55\n   3.    61.25  28.95   4.57   4.6   18.36  64.75  30.37   5.09  21.33\n  35.84   3.     6.75  30.85   4.     6.25   5.5   17.08   4.25   7.9\n   5.65   2.2   10.     4.75   7.39   5.75   9.5   11.66   5.5   10.07\n  16.95  21.5    2.75   6.94   6.25   4.8    3.8    4.75  10.     9.66\n   3.8    3.7    5.85   4.29   8.     5.07   8.25  25.75   9.44  35.\n  11.85  13.2    4.6   11.86  22.99   7.29   5.9    3.4    8.     4.\n  12.85  26.76   3.5    5.75   7.5   13.5    5.11  16.52   4.     3.25\n   7.43   3.2    4.75].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-34a030eefc26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'New_Price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mregressor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rbf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\svm\\base.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m         X, y = check_X_y(X, y, dtype=np.float64,\n\u001b[0;32m    148\u001b[0m                          \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                          accept_large_sparse=False)\n\u001b[0m\u001b[0;32m    150\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    550\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m                     \u001b[1;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[  4.5   17.5    9.95  15.    18.55   9.9    6.98   8.63   8.85  10.95\n  16.5   35.67  10.5    6.92   4.5    3.95   3.9   19.25   3.91  20.75\n   6.55   7.5   10.95   2.6   54.    11.5   37.    26.5    6.5   33.5\n   7.5    8.5    6.5    9.     3.25  26.     3.7    2.65  35.67  19.64\n   3.87  18.65   2.6   61.29   3.75   5.25   4.85  20.25  12.69  57.\n   3.13  62.67   7.15  25.99   2.05   3.75  52.    27.3   35.     9.49\n   9.48  34.5   28.    25.5    8.5    6.9   70.99  24.5   28.5    7.7\n   7.1    8.92  20.37  36.5   51.43  56.     3.3    8.    38.99  29.4\n   6.75  10.5   17.    19.     6.     8.12  24.82  48.5    9.45  17.5\n   8.9   29.5   11.25  10.79   5.7    2.75   9.25  11.31   5.43  18.75\n   8.5    4.9    5.7    7.75   3.2   23.5   10.65   8.46   2.5   10.9\n   4.32  17.63  56.     8.5    2.3    6.45   7.5   18.6    5.9   19.92\n   3.25   5.     2.7    2.66   3.61   8.65  20.    59.72   6.5   12.49\n  18.39   6.21  39.5   17.5    4.45  11.65  16.5    5.39  13.28  17.75\n  25.    12.41   7.94  20.5    7.    10.49   3.95   6.5    7.25   6.01\n  12.     5.88  51.35  22.     5.    19.75  11.44   3.5   10.46   5.68\n   5.98   4.77   7.2    5.9   25.    14.    20.4    5.68  12.75  18.\n   6.5   40.     6.15   6.24   5.5   11.97  18.9    2.95   5.25  24.68\n  29.5    1.9    2.5   13.95   6.06   2.96  67.     3.6    7.55   6.17\n   2.5   11.5    6.2   15.49   4.    19.92   9.2    3.5    4.5   16.35\n  12.5    5.8    5.    17.49  97.07   9.15   7.7    6.69  10.5   22.55\n   9.25   7.29  23.5    3.1    7.23  41.5   24.9   72.94   5.5   30.29\n  38.     2.74   4.9    3.22  37.3   70.8    6.03   7.8   28.03   5.\n   8.2    2.75   4.74   4.25   9.29   5.23   6.01   5.     9.     7.27\n   5.1    7.25   5.25   8.9   17.45   6.22  64.87  17.25   5.53  10.9\n   2.9   19.65  79.     5.75   7.27  30.     5.75   8.09  22.24   8.99\n   8.2    5.25   9.3   23.5    4.25  17.02   7.12  11.25   3.67  28.5\n  16.5    6.76   5.5    3.75  17.11   4.3    6.36  43.6    3.05  19.05\n  34.     4.     8.41   5.89   4.     3.94   5.25  83.96  21.     4.3\n  56.73   6.     7.53   5.8   31.     4.85   5.65  11.    11.6    3.95\n  41.6   23.     8.    17.75   3.6    1.75   5.99   6.71  14.21   5.11\n  22.99   3.6   13.7    3.1   12.9   17.5    8.45  10.75   5.22   9.18\n  13.9    3.25  78.8    9.69   5.65  22.5   28.9   22.    21.5    3.9\n   8.11   7.65   6.     7.25   9.     5.92   6.29  17.99   4.85   4.\n   3.6   21.75  45.     3.19   6.2   10.25  30.     8.99   4.5    9.38\n  17.85   3.75   8.95  56.5   21.    36.75  27.87   8.75  11.92   7.57\n   3.48  21.69  20.14   4.15   3.15   6.97  35.    11.5    6.5    8.\n   5.99   3.63   4.25   3.36  10.3    3.41  27.5    3.5    4.58  10.34\n   5.47  35.98   7.9    9.51  19.5    9.13   2.25   2.35  17.92  59.29\n   6.75   8.25   6.07   4.28  64.95   5.55  17.98   4.5    7.75  33.13\n   4.8    6.     9.92  24.     9.6   25.    10.44   9.9   20.     6.5\n   5.85  23.91   7.36  17.65   6.    18.25   4.25  16.9    5.     5.49\n   3.25   9.25   4.5    2.8   18.5    5.95   5.36   3.5    6.67   5.85\n  16.95   4.68   3.46  13.08   4.     9.51   3.25   2.02   3.95   6.84\n  21.73  31.55  27.36   4.65   2.25   3.72   3.45   6.08  18.    24.77\n  13.    26.5    8.95   7.7    8.5    2.5    4.65  17.5    7.34   5.9\n  27.    48.    59.65   7.29  23.8    5.65  25.75   3.85   3.5   11.15\n   8.9    8.61   9.25   9.25  10.15   3.75   8.25   4.4    4.5   10.3\n   5.5   21.26  11.5   12.5   12.46   5.1    6.3    5.8   20.    12.\n   5.95  22.2   30.77   6.25  39.84   5.74  35.     8.9   19.46   9.22\n   3.5   10.64   6.95   6.99  14.25   4.3    5.5    8.27  16.5   50.97\n  29.95   9.99   9.25   5.45   5.25  14.2    4.87  11.39   8.    70.66\n   3.25   9.18   4.11  19.97   4.5    5.85   2.15  41.5   11.     8.7\n   9.9    5.77  11.5    4.95  16.45   8.7   20.75   8.25  19.    17.55\n   3.     8.25   4.69   4.95   3.2   24.5   11.25  40.5   22.45 160.\n   8.61  26.11   6.5   16.81   3.41   2.75  35.82   8.02   2.85   6.67\n  20.5   11.31   3.6   11.45   6.37   8.2    5.49   7.7    7.75   2.93\n  32.73  21.66   2.99   3.15   3.31  17.85  19.6    4.45   8.85   7.56\n  18.    29.     5.87   7.75  21.42   4.1   16.5    5.     6.7    2.45\n   4.8    5.66  56.8   19.4    5.99   3.3    4.5   15.5    3.4    7.97\n   8.55   3.75  16.42  65.     3.95  10.5   17.27  27.     6.8    3.16\n  11.5    3.     9.75  36.64   5.43   3.     8.9    3.59   7.64   7.64\n  28.85   5.75  19.5    5.    11.88   4.     2.9    5.85  42.     6.28\n  30.    10.8   24.    37.92   7.25   9.66   2.25   3.46  13.95   2.8\n  43.     0.7    6.95   9.62   3.01  10.19   8.4    8.85  10.    46.95\n  13.25   9.7    8.37  24.37   6.     8.3    5.     2.9    8.86   7.9\n  16.27  48.    18.9    4.91  32.57  68.     8.4    8.39  56.14  15.\n   9.05   3.75   9.88   8.78   5.5    2.1    5.5    5.34  49.     3.75\n  60.75   2.65   5.27   7.4   27.    10.77  11.75   3.8    4.52   6.26\n   5.25   4.99   4.41   8.14   8.45  12.5    7.69  12.13   8.75   7.23\n   7.5    8.9    4.9    3.69   3.98   6.69   9.5    5.63  11.78   4.8\n   3.49   6.3    8.15  35.     4.5    6.9   35.5    6.28   3.7    2.9\n   4.25  29.5   10.9    4.3    8.6    6.75   7.4    7.21   4.2   16.5\n   5.15   3.7    2.25   8.49   5.5   11.99  29.99   4.25   8.25   8.2\n  24.46   3.9    3.65   5.     3.85  11.5    4.5   13.82   6.     2.55\n   3.    61.25  28.95   4.57   4.6   18.36  64.75  30.37   5.09  21.33\n  35.84   3.     6.75  30.85   4.     6.25   5.5   17.08   4.25   7.9\n   5.65   2.2   10.     4.75   7.39   5.75   9.5   11.66   5.5   10.07\n  16.95  21.5    2.75   6.94   6.25   4.8    3.8    4.75  10.     9.66\n   3.8    3.7    5.85   4.29   8.     5.07   8.25  25.75   9.44  35.\n  11.85  13.2    4.6   11.86  22.99   7.29   5.9    3.4    8.     4.\n  12.85  26.76   3.5    5.75   7.5   13.5    5.11  16.52   4.     3.25\n   7.43   3.2    4.75].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "# most important SVR parameter is Kernel type. It can be #linear,polynomial or gaussian SVR. We have a non-linear condition #so we can select polynomial or gaussian but here we select RBF(a #gaussian type) kernel.\n",
    "X=cars['Price']\n",
    "y=cars['New_Price']\n",
    "regressor = SVR(kernel='rbf')\n",
    "regressor.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-fffda561571a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'magenta'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'green'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Truth or Bluff (Support Vector Regression Model)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Position level'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CarPrice'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\matplotlib\\pyplot.pyc\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, hold, data, **kwargs)\u001b[0m\n\u001b[0;32m   3473\u001b[0m                          \u001b[0mvmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvmax\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3474\u001b[0m                          \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlinewidths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3475\u001b[1;33m                          edgecolors=edgecolors, data=data, **kwargs)\n\u001b[0m\u001b[0;32m   3476\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3477\u001b[0m         \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\matplotlib\\__init__.pyc\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1865\u001b[0m                         \u001b[1;34m\"the Matplotlib list!)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlabel_namer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m                         RuntimeWarning, stacklevel=2)\n\u001b[1;32m-> 1867\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         inner.__doc__ = _add_data_doc(inner.__doc__,\n",
      "\u001b[1;32mC:\\Users\\Amardeep\\Anaconda2\\lib\\site-packages\\matplotlib\\axes\\_axes.pyc\u001b[0m in \u001b[0;36mscatter\u001b[1;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, **kwargs)\u001b[0m\n\u001b[0;32m   4255\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4257\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x and y must be the same size\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X, y, color = 'magenta')\n",
    "plt.plot(X, regressor.predict(X), color = 'green')\n",
    "plt.title('Truth or Bluff (Support Vector Regression Model)')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('CarPrice')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
